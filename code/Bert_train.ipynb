{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Model\n",
    "\n",
    "Fine-tune a Bert model to classify sentiment.\n",
    "\n",
    "-----\n",
    "```\n",
    ": 25.05.24\n",
    ": Zach Wolpe\n",
    ": zachcolinwolpe@gmail.com\n",
    "```\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zachwolpe/miniforge3/envs/mlxgo/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/zachwolpe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importing neccessary libraries \n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertPreTrainedModel, BertModel,AdamW\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import argparse\n",
    "import logging\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import torch.optim as optim\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/zachwolpe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from Bert_classifier import (BertWithActivationAndRegularization, Bert_tokenize)\n",
    "from ML_training_code import (generate_K_Fold_data, torch_tensorize, plot_training_validation, training_loop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Config\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config ---------------------------------------->>\n",
    "PATH_TO_DATA = '../data/train_test/'\n",
    "SAVE_LOC = '../model-artifacts/'\n",
    "BATCH_SIZE = 10\n",
    "DEBUG_MODE = True\n",
    "EPOCHS = 1\n",
    "CROSS_VALIDATION = True\n",
    "K_FOLDS = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Config ---------------------------------------->>\n",
    "\n",
    "# Load data\n",
    "X_train = pd.read_csv(PATH_TO_DATA + 'X_train.csv')\n",
    "y_train = pd.read_csv(PATH_TO_DATA + 'y_train.csv')\n",
    "X_test = pd.read_csv(PATH_TO_DATA + 'X_test.csv')\n",
    "y_test = pd.read_csv(PATH_TO_DATA + 'y_test.csv')\n",
    "\n",
    "\n",
    "# Downsample for testing\n",
    "if DEBUG_MODE:\n",
    "    X_train = X_train[:10]\n",
    "    y_train = y_train[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Instantiate Bert\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertWithActivationAndRegularization(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (activation): GELU(approximate='none')\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the pretrained BERT model name\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "# Create an instance of the model\n",
    "model = BertWithActivationAndRegularization(pretrained_model_name='bert-base-uncased', num_labels=3)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Training Runtime Hyperparameters\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer and criterion\n",
    "learning_rate = 2e-06\n",
    "weight_decay = 0.0001  \n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Run without K-Fold Cross Validation\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 1 fold\n",
    "if not CROSS_VALIDATION:\n",
    "    gkfd = generate_K_Fold_data(X_train, y_train, num_splits=K_FOLDS)\n",
    "    X_train_fold, y_train_fold, X_val_fold, y_val_fold, train_index, val_index = next(gkfd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Tokenize\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CROSS_VALIDATION:\n",
    "    input_ids_train, attention_masks_train, y_train = Bert_tokenize(X_train_fold, y_train_fold)\n",
    "    input_ids_val, attention_masks_val, y_val = Bert_tokenize(X_val_fold, y_val_fold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Torch Tensors\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CROSS_VALIDATION:\n",
    "        (train_dataloader,\n",
    "        train_dataloader,\n",
    "        val_dataset,\n",
    "        val_dataloader) = torch_tensorize(input_ids_train,\n",
    "                attention_masks_train,\n",
    "                y_train_fold,\n",
    "                input_ids_val,\n",
    "                attention_masks_val,\n",
    "                y_val_fold,\n",
    "                BATCH_SIZE=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Loop\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CROSS_VALIDATION:\n",
    "    # Training loop\n",
    "    train_losses_fold, valid_losses_fold, accuracies_fold = training_loop(model,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        device,\n",
    "        epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Plot training and validation loss\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CROSS_VALIDATION:\n",
    "    plot_training_validation(train_losses_fold, valid_losses_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## K-Fold Cross-Validation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Training the model...\n",
      "Epoch 1/1 - Training Loss: 1.2094 - Validation Loss: 1.0610 - Training Accuracy: 0.2500 - Validation Accuracy: 0.5000\n",
      "Finished Training.\n",
      "Fold 2/5\n",
      "Training the model...\n",
      "Epoch 1/1 - Training Loss: 1.2460 - Validation Loss: 1.3418 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
      "Finished Training.\n",
      "Fold 3/5\n",
      "Training the model...\n",
      "Epoch 1/1 - Training Loss: 1.2759 - Validation Loss: 1.1019 - Training Accuracy: 0.1250 - Validation Accuracy: 0.5000\n",
      "Finished Training.\n",
      "Fold 4/5\n",
      "Training the model...\n",
      "Epoch 1/1 - Training Loss: 1.2752 - Validation Loss: 1.1393 - Training Accuracy: 0.2500 - Validation Accuracy: 0.5000\n",
      "Finished Training.\n",
      "Fold 5/5\n",
      "Training the model...\n",
      "Epoch 1/1 - Training Loss: 1.1027 - Validation Loss: 1.3828 - Training Accuracy: 0.3750 - Validation Accuracy: 0.0000\n",
      "Finished Training.\n"
     ]
    }
   ],
   "source": [
    "if CROSS_VALIDATION:\n",
    "    \n",
    "    # Build generator\n",
    "    gkfd = generate_K_Fold_data(X_train, y_train, num_splits=K_FOLDS)\n",
    "    \n",
    "    results = []\n",
    "    for i, _gkfd in enumerate(gkfd):\n",
    "        print(f'Fold {i+1}/{K_FOLDS}')\n",
    "        X_train_fold, y_train_fold, X_val_fold, y_val_fold, train_index, val_index = _gkfd\n",
    "\n",
    "        # Tokenize\n",
    "        input_ids_train, attention_masks_train, y_train = Bert_tokenize(X_train_fold, y_train_fold)\n",
    "        input_ids_val, attention_masks_val, y_val = Bert_tokenize(X_val_fold, y_val_fold)\n",
    "\n",
    "\n",
    "        # Create Torch Tensors\n",
    "        (train_dataloader,\n",
    "        train_dataloader,\n",
    "        val_dataset,\n",
    "        val_dataloader) = torch_tensorize(input_ids_train,\n",
    "                attention_masks_train,\n",
    "                y_train_fold,\n",
    "                input_ids_val,\n",
    "                attention_masks_val,\n",
    "                y_val_fold,\n",
    "                BATCH_SIZE=BATCH_SIZE)\n",
    "\n",
    "        # Define lists to store metrics for each fold\n",
    "        train_losses_per_fold = []\n",
    "        valid_losses_per_fold = []\n",
    "        accuracies_per_fold = []\n",
    "\n",
    "        # Training loop\n",
    "        train_losses_fold, valid_losses_fold, accuracies_fold = training_loop(model,\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            train_dataloader,\n",
    "            val_dataloader,\n",
    "            device,\n",
    "            epochs=EPOCHS)\n",
    "        \n",
    "        # Save metrics for this fold\n",
    "        train_losses_per_fold.append(train_losses_fold)\n",
    "        valid_losses_per_fold.append(valid_losses_fold)\n",
    "        accuracies_per_fold.append(accuracies_fold)\n",
    "\n",
    "        # save results\n",
    "        results.append({\n",
    "            'train_losses': train_losses_fold,\n",
    "            'valid_losses': valid_losses_fold,\n",
    "            'accuracies': accuracies_fold\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Save Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Bert Model\n",
    "if not DEBUG_MODE:\n",
    "    torch.save(model.state_dict(), f'{SAVE_LOC}bert_model.pth')\n",
    "\n",
    "else:\n",
    "    torch.save(model.state_dict(), f'{SAVE_LOC}bert_model_debug_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
