{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Model\n",
    "\n",
    "\n",
    "----\n",
    "```\n",
    ": 26.05.24\n",
    ": zachcolinwolpe@gmail.com\n",
    "```\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/zachwolpe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importing neccessary libraries \n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertPreTrainedModel, BertModel,AdamW\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import argparse\n",
    "import logging\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import torch.optim as optim\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/zachwolpe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from Roberta_classifier import (RobertaWithActivationAndRegularization, RoBertaTokenizer)\n",
    "from ML_training_code import (generate_K_Fold_data, torch_tensorize, plot_training_validation, training_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bert_classifier import (BertWithActivationAndRegularization, Bert_tokenize)\n",
    "from ML_training_code import (generate_K_Fold_data, torch_tensorize, plot_training_validation, training_loop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Pre-Trained Models\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1422, 1), (1422, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Config ---------------------------------------->>\n",
    "PATH_TO_DATA = '../data/train_test/'\n",
    "SAVE_LOC = '../model-artifacts/'\n",
    "BERT_MODEL_NAME = 'bert_model_debug_model.pth'\n",
    "ROBERTA_MODEL_NAME = 'roberta_model_debug_model.pth'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEBUG_MODE = True\n",
    "BATCH_SIZE = 10\n",
    "# Config ---------------------------------------->>\n",
    "\n",
    "\n",
    "# -+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
    "# Thelma: updated params -+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
    "SAVE_LOC = '../thelma-models/'\n",
    "BERT_MODEL_NAME = 'bert-base-uncased.pth'\n",
    "ROBERTA_MODEL_NAME = 'roberta-base.pth'\n",
    "DEBUG_MODE = False\n",
    "# Thelma: updated params -+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
    "# -+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
    "\n",
    "# Load data\n",
    "X_test = pd.read_csv(PATH_TO_DATA + 'X_test.csv')\n",
    "y_test = pd.read_csv(PATH_TO_DATA + 'y_test.csv')\n",
    "\n",
    "\n",
    "# Downsample for testing\n",
    "if DEBUG_MODE:\n",
    "    X_test = X_test[:3]\n",
    "    y_test = y_test[:3]\n",
    "\n",
    "\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Load Models\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load Bert model\n",
    "Bert_model = BertWithActivationAndRegularization(pretrained_model_name='bert-base-uncased', num_labels=3)\n",
    "\n",
    "try:\n",
    "    _bert_state_keys = torch.load(f'{SAVE_LOC}{BERT_MODEL_NAME}')\n",
    "# Bert_model\n",
    "\n",
    "except:\n",
    "    _bert_state_keys = torch.load(f'{SAVE_LOC}{BERT_MODEL_NAME}')\n",
    "    Bert_model.load_state_dict(_bert_state_keys['model_state_dict'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load RoBerta model\n",
    "RoBerta_model = RobertaWithActivationAndRegularization(pretrained_model_name='roberta-base', num_labels=3,)\n",
    "\n",
    "try:\n",
    "    RoBerta_model.load_state_dict(torch.load(f'{SAVE_LOC}{ROBERTA_MODEL_NAME}'))\n",
    "except:\n",
    "    _roberta_state_keys = torch.load(f'{SAVE_LOC}{ROBERTA_MODEL_NAME}')\n",
    "    RoBerta_model.load_state_dict(_roberta_state_keys['model_state_dict'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Inference Engine\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inference(model, X_test, y_test, tokenizer=Bert_tokenize, criterion=nn.CrossEntropyLoss()):\n",
    "    \"\"\"\n",
    "    Perform inference on the model\n",
    "\n",
    "    Return softmax probabilities, predictions, and metrics\n",
    "    \"\"\"    \n",
    "    # evaluate on test data\n",
    "    model.eval()\n",
    "    input_ids_test, attention_masks_test, y_test = tokenizer(X_test, y_test)\n",
    "    test_dataset = TensorDataset(input_ids_test, attention_masks_test, y_test)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    total_test_loss = 0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "\n",
    "    #return items\n",
    "    softmax_prob = []\n",
    "    predictions = []\n",
    "    accuracy = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            labels = labels.reshape(-1)  # Reshape labels once\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs\n",
    "            loss = criterion(logits, labels)\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "            total_test += labels.size(0)\n",
    "            accuracy_test = correct_test / total_test\n",
    "            print(f'Test Loss: {total_test_loss:.4f} - Test Accuracy: {accuracy_test:.4f}')\n",
    "\n",
    "            # Get softmax probabilities\n",
    "            softmax_prob.append(nn.functional.softmax(logits, dim=1).cpu().numpy())\n",
    "            predictions.append(predicted.cpu().numpy())\n",
    "            accuracy.append(accuracy_test)\n",
    "\n",
    "        # flatten the list\n",
    "        softmax_prob = np.concatenate(softmax_prob)\n",
    "        predictions = np.concatenate(predictions)\n",
    "        accuracy = np.mean(accuracy)\n",
    "        \n",
    "        return softmax_prob, predictions, accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Model Evaluation: Bert \n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.9119 - Test Accuracy: 0.8000\n",
      "Test Loss: 1.4482 - Test Accuracy: 0.7500\n",
      "Test Loss: 2.2482 - Test Accuracy: 0.7333\n",
      "Test Loss: 2.3505 - Test Accuracy: 0.8000\n",
      "Test Loss: 3.3951 - Test Accuracy: 0.7800\n",
      "Test Loss: 3.6045 - Test Accuracy: 0.8000\n",
      "Test Loss: 4.0885 - Test Accuracy: 0.8143\n",
      "Test Loss: 5.4044 - Test Accuracy: 0.8000\n",
      "Test Loss: 6.7665 - Test Accuracy: 0.7889\n",
      "Test Loss: 7.6394 - Test Accuracy: 0.7800\n",
      "Test Loss: 8.3547 - Test Accuracy: 0.7727\n",
      "Test Loss: 8.6734 - Test Accuracy: 0.7750\n",
      "Test Loss: 9.2528 - Test Accuracy: 0.7769\n",
      "Test Loss: 10.1618 - Test Accuracy: 0.7714\n",
      "Test Loss: 10.9091 - Test Accuracy: 0.7667\n",
      "Test Loss: 11.2582 - Test Accuracy: 0.7750\n",
      "Test Loss: 11.7147 - Test Accuracy: 0.7706\n",
      "Test Loss: 12.1509 - Test Accuracy: 0.7722\n",
      "Test Loss: 12.2152 - Test Accuracy: 0.7842\n",
      "Test Loss: 12.8568 - Test Accuracy: 0.7850\n",
      "Test Loss: 13.4216 - Test Accuracy: 0.7857\n",
      "Test Loss: 14.4621 - Test Accuracy: 0.7818\n",
      "Test Loss: 14.7104 - Test Accuracy: 0.7870\n",
      "Test Loss: 16.1644 - Test Accuracy: 0.7833\n",
      "Test Loss: 17.5677 - Test Accuracy: 0.7800\n",
      "Test Loss: 17.6759 - Test Accuracy: 0.7885\n",
      "Test Loss: 18.4853 - Test Accuracy: 0.7889\n",
      "Test Loss: 19.0570 - Test Accuracy: 0.7929\n",
      "Test Loss: 19.7141 - Test Accuracy: 0.7931\n",
      "Test Loss: 20.4130 - Test Accuracy: 0.7900\n",
      "Test Loss: 20.5129 - Test Accuracy: 0.7968\n",
      "Test Loss: 21.0824 - Test Accuracy: 0.7969\n",
      "Test Loss: 21.8777 - Test Accuracy: 0.7970\n",
      "Test Loss: 23.1221 - Test Accuracy: 0.7912\n",
      "Test Loss: 23.5044 - Test Accuracy: 0.7914\n",
      "Test Loss: 24.0005 - Test Accuracy: 0.7944\n",
      "Test Loss: 24.2824 - Test Accuracy: 0.7973\n",
      "Test Loss: 25.2447 - Test Accuracy: 0.7947\n",
      "Test Loss: 25.7272 - Test Accuracy: 0.7974\n",
      "Test Loss: 27.9010 - Test Accuracy: 0.7875\n",
      "Test Loss: 28.2641 - Test Accuracy: 0.7902\n",
      "Test Loss: 28.5425 - Test Accuracy: 0.7905\n",
      "Test Loss: 28.5844 - Test Accuracy: 0.7953\n",
      "Test Loss: 28.6742 - Test Accuracy: 0.7977\n",
      "Test Loss: 28.9504 - Test Accuracy: 0.7978\n",
      "Test Loss: 29.6889 - Test Accuracy: 0.7978\n",
      "Test Loss: 29.8353 - Test Accuracy: 0.8021\n",
      "Test Loss: 31.0967 - Test Accuracy: 0.8000\n",
      "Test Loss: 31.6741 - Test Accuracy: 0.8000\n",
      "Test Loss: 32.2325 - Test Accuracy: 0.8000\n",
      "Test Loss: 32.9622 - Test Accuracy: 0.8000\n",
      "Test Loss: 33.5509 - Test Accuracy: 0.8000\n",
      "Test Loss: 33.9585 - Test Accuracy: 0.8019\n",
      "Test Loss: 35.0481 - Test Accuracy: 0.8000\n",
      "Test Loss: 35.8526 - Test Accuracy: 0.8000\n",
      "Test Loss: 36.2512 - Test Accuracy: 0.8018\n",
      "Test Loss: 36.9795 - Test Accuracy: 0.8018\n",
      "Test Loss: 37.5284 - Test Accuracy: 0.8017\n",
      "Test Loss: 37.6043 - Test Accuracy: 0.8051\n",
      "Test Loss: 37.6738 - Test Accuracy: 0.8083\n",
      "Test Loss: 38.0300 - Test Accuracy: 0.8098\n",
      "Test Loss: 39.9665 - Test Accuracy: 0.8048\n",
      "Test Loss: 40.7594 - Test Accuracy: 0.8048\n",
      "Test Loss: 40.8686 - Test Accuracy: 0.8063\n",
      "Test Loss: 41.9151 - Test Accuracy: 0.8031\n",
      "Test Loss: 42.6660 - Test Accuracy: 0.8015\n",
      "Test Loss: 43.0594 - Test Accuracy: 0.8030\n",
      "Test Loss: 43.5590 - Test Accuracy: 0.8029\n",
      "Test Loss: 44.0567 - Test Accuracy: 0.8043\n",
      "Test Loss: 44.1807 - Test Accuracy: 0.8057\n",
      "Test Loss: 45.7911 - Test Accuracy: 0.8028\n",
      "Test Loss: 46.2610 - Test Accuracy: 0.8028\n",
      "Test Loss: 46.2937 - Test Accuracy: 0.8055\n",
      "Test Loss: 47.1049 - Test Accuracy: 0.8054\n",
      "Test Loss: 47.6139 - Test Accuracy: 0.8053\n",
      "Test Loss: 47.9927 - Test Accuracy: 0.8066\n",
      "Test Loss: 48.1432 - Test Accuracy: 0.8091\n",
      "Test Loss: 48.9983 - Test Accuracy: 0.8090\n",
      "Test Loss: 49.5280 - Test Accuracy: 0.8101\n",
      "Test Loss: 50.6218 - Test Accuracy: 0.8063\n",
      "Test Loss: 51.6237 - Test Accuracy: 0.8037\n",
      "Test Loss: 52.5516 - Test Accuracy: 0.8024\n",
      "Test Loss: 53.7382 - Test Accuracy: 0.8000\n",
      "Test Loss: 53.8344 - Test Accuracy: 0.8024\n",
      "Test Loss: 53.9953 - Test Accuracy: 0.8035\n",
      "Test Loss: 55.0798 - Test Accuracy: 0.8012\n",
      "Test Loss: 55.3748 - Test Accuracy: 0.8023\n",
      "Test Loss: 55.4659 - Test Accuracy: 0.8045\n",
      "Test Loss: 56.7929 - Test Accuracy: 0.8034\n",
      "Test Loss: 57.1217 - Test Accuracy: 0.8033\n",
      "Test Loss: 57.2711 - Test Accuracy: 0.8044\n",
      "Test Loss: 57.7018 - Test Accuracy: 0.8054\n",
      "Test Loss: 58.4695 - Test Accuracy: 0.8043\n",
      "Test Loss: 58.9725 - Test Accuracy: 0.8043\n",
      "Test Loss: 59.1286 - Test Accuracy: 0.8053\n",
      "Test Loss: 59.7111 - Test Accuracy: 0.8052\n",
      "Test Loss: 60.2758 - Test Accuracy: 0.8052\n",
      "Test Loss: 61.0456 - Test Accuracy: 0.8041\n",
      "Test Loss: 61.4328 - Test Accuracy: 0.8030\n",
      "Test Loss: 61.9382 - Test Accuracy: 0.8030\n",
      "Test Loss: 62.7261 - Test Accuracy: 0.8020\n",
      "Test Loss: 63.2798 - Test Accuracy: 0.8020\n",
      "Test Loss: 63.2987 - Test Accuracy: 0.8039\n",
      "Test Loss: 63.9997 - Test Accuracy: 0.8019\n",
      "Test Loss: 64.0811 - Test Accuracy: 0.8038\n",
      "Test Loss: 64.4998 - Test Accuracy: 0.8047\n",
      "Test Loss: 65.0909 - Test Accuracy: 0.8037\n",
      "Test Loss: 65.2297 - Test Accuracy: 0.8046\n",
      "Test Loss: 65.8439 - Test Accuracy: 0.8055\n",
      "Test Loss: 65.9893 - Test Accuracy: 0.8073\n",
      "Test Loss: 66.8809 - Test Accuracy: 0.8072\n",
      "Test Loss: 67.1048 - Test Accuracy: 0.8080\n",
      "Test Loss: 68.3099 - Test Accuracy: 0.8071\n",
      "Test Loss: 68.7859 - Test Accuracy: 0.8079\n",
      "Test Loss: 69.1100 - Test Accuracy: 0.8087\n",
      "Test Loss: 70.4422 - Test Accuracy: 0.8069\n",
      "Test Loss: 71.0561 - Test Accuracy: 0.8077\n",
      "Test Loss: 71.5002 - Test Accuracy: 0.8085\n",
      "Test Loss: 72.0591 - Test Accuracy: 0.8092\n",
      "Test Loss: 72.7987 - Test Accuracy: 0.8092\n",
      "Test Loss: 73.4828 - Test Accuracy: 0.8091\n",
      "Test Loss: 73.7047 - Test Accuracy: 0.8090\n",
      "Test Loss: 74.3393 - Test Accuracy: 0.8073\n",
      "Test Loss: 74.4139 - Test Accuracy: 0.8089\n",
      "Test Loss: 74.5138 - Test Accuracy: 0.8104\n",
      "Test Loss: 75.2666 - Test Accuracy: 0.8103\n",
      "Test Loss: 76.5420 - Test Accuracy: 0.8087\n",
      "Test Loss: 76.5828 - Test Accuracy: 0.8102\n",
      "Test Loss: 77.1313 - Test Accuracy: 0.8101\n",
      "Test Loss: 77.6650 - Test Accuracy: 0.8108\n",
      "Test Loss: 77.7296 - Test Accuracy: 0.8122\n",
      "Test Loss: 78.0104 - Test Accuracy: 0.8129\n",
      "Test Loss: 78.6092 - Test Accuracy: 0.8135\n",
      "Test Loss: 79.5586 - Test Accuracy: 0.8134\n",
      "Test Loss: 80.1797 - Test Accuracy: 0.8133\n",
      "Test Loss: 80.7192 - Test Accuracy: 0.8132\n",
      "Test Loss: 81.2769 - Test Accuracy: 0.8131\n",
      "Test Loss: 81.7510 - Test Accuracy: 0.8138\n",
      "Test Loss: 82.5939 - Test Accuracy: 0.8129\n",
      "Test Loss: 83.5499 - Test Accuracy: 0.8121\n",
      "Test Loss: 83.5987 - Test Accuracy: 0.8135\n",
      "Test Loss: 84.3058 - Test Accuracy: 0.8127\n",
      "Test Loss: 84.7302 - Test Accuracy: 0.8122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.01167926, 0.9558555 , 0.03246526],\n",
       "        [0.00395421, 0.0040915 , 0.99195427],\n",
       "        [0.03429046, 0.76527894, 0.20043065],\n",
       "        ...,\n",
       "        [0.00571223, 0.0073569 , 0.98693085],\n",
       "        [0.9406599 , 0.05330991, 0.00603017],\n",
       "        [0.5350184 , 0.45488903, 0.01009255]], dtype=float32),\n",
       " array([1, 2, 1, ..., 2, 0, 0]),\n",
       " 0.7997736432553271)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(softmax_prob_Bert,\n",
    " predictions_Bert,\n",
    " accuracy_Bert) = inference(Bert_model,\n",
    "                            X_test,\n",
    "                            y_test,\n",
    "                            tokenizer=Bert_tokenize)\n",
    "softmax_prob_Bert, predictions_Bert, accuracy_Bert\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.01167926, 0.9558555 , 0.03246526],\n",
       "        [0.00395421, 0.0040915 , 0.99195427],\n",
       "        [0.03429046, 0.76527894, 0.20043065],\n",
       "        ...,\n",
       "        [0.00571223, 0.0073569 , 0.98693085],\n",
       "        [0.9406599 , 0.05330991, 0.00603017],\n",
       "        [0.5350184 , 0.45488903, 0.01009255]], dtype=float32),\n",
       " 0.7997736432553271)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_prob_Bert, accuracy_Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Model Evaluation: RoBerta \n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.3179 - Test Accuracy: 0.7000\n",
      "Test Loss: 2.4354 - Test Accuracy: 0.6500\n",
      "Test Loss: 3.4487 - Test Accuracy: 0.6667\n",
      "Test Loss: 4.2255 - Test Accuracy: 0.7000\n",
      "Test Loss: 5.4550 - Test Accuracy: 0.7000\n",
      "Test Loss: 5.4869 - Test Accuracy: 0.7500\n",
      "Test Loss: 5.4962 - Test Accuracy: 0.7857\n",
      "Test Loss: 7.0697 - Test Accuracy: 0.7625\n",
      "Test Loss: 7.8033 - Test Accuracy: 0.7778\n",
      "Test Loss: 9.3687 - Test Accuracy: 0.7600\n",
      "Test Loss: 9.7111 - Test Accuracy: 0.7727\n",
      "Test Loss: 9.7933 - Test Accuracy: 0.7917\n",
      "Test Loss: 10.3659 - Test Accuracy: 0.8000\n",
      "Test Loss: 11.3466 - Test Accuracy: 0.7929\n",
      "Test Loss: 12.3766 - Test Accuracy: 0.7867\n",
      "Test Loss: 12.7735 - Test Accuracy: 0.7937\n",
      "Test Loss: 13.3210 - Test Accuracy: 0.8000\n",
      "Test Loss: 13.9617 - Test Accuracy: 0.8000\n",
      "Test Loss: 14.0921 - Test Accuracy: 0.8053\n",
      "Test Loss: 14.5240 - Test Accuracy: 0.8100\n",
      "Test Loss: 15.0799 - Test Accuracy: 0.8143\n",
      "Test Loss: 16.2730 - Test Accuracy: 0.8045\n",
      "Test Loss: 16.5901 - Test Accuracy: 0.8087\n",
      "Test Loss: 18.3639 - Test Accuracy: 0.7958\n",
      "Test Loss: 19.7793 - Test Accuracy: 0.7920\n",
      "Test Loss: 20.8022 - Test Accuracy: 0.7923\n",
      "Test Loss: 21.5437 - Test Accuracy: 0.7926\n",
      "Test Loss: 22.6232 - Test Accuracy: 0.7893\n",
      "Test Loss: 23.7301 - Test Accuracy: 0.7862\n",
      "Test Loss: 24.0294 - Test Accuracy: 0.7900\n",
      "Test Loss: 24.4472 - Test Accuracy: 0.7903\n",
      "Test Loss: 25.5670 - Test Accuracy: 0.7875\n",
      "Test Loss: 26.7950 - Test Accuracy: 0.7818\n",
      "Test Loss: 27.6749 - Test Accuracy: 0.7824\n",
      "Test Loss: 28.0767 - Test Accuracy: 0.7857\n",
      "Test Loss: 28.5018 - Test Accuracy: 0.7889\n",
      "Test Loss: 28.5239 - Test Accuracy: 0.7946\n",
      "Test Loss: 29.5007 - Test Accuracy: 0.7947\n",
      "Test Loss: 29.7322 - Test Accuracy: 0.7974\n",
      "Test Loss: 32.1651 - Test Accuracy: 0.7900\n",
      "Test Loss: 32.7026 - Test Accuracy: 0.7927\n",
      "Test Loss: 33.2698 - Test Accuracy: 0.7952\n",
      "Test Loss: 33.3121 - Test Accuracy: 0.8000\n",
      "Test Loss: 33.4570 - Test Accuracy: 0.8023\n",
      "Test Loss: 33.7846 - Test Accuracy: 0.8022\n",
      "Test Loss: 35.1685 - Test Accuracy: 0.8000\n",
      "Test Loss: 35.1898 - Test Accuracy: 0.8043\n",
      "Test Loss: 36.1856 - Test Accuracy: 0.8042\n",
      "Test Loss: 37.1566 - Test Accuracy: 0.8041\n",
      "Test Loss: 37.3459 - Test Accuracy: 0.8060\n",
      "Test Loss: 38.0147 - Test Accuracy: 0.8078\n",
      "Test Loss: 38.5940 - Test Accuracy: 0.8096\n",
      "Test Loss: 39.2030 - Test Accuracy: 0.8113\n",
      "Test Loss: 39.9431 - Test Accuracy: 0.8111\n",
      "Test Loss: 41.0682 - Test Accuracy: 0.8109\n",
      "Test Loss: 41.8595 - Test Accuracy: 0.8125\n",
      "Test Loss: 42.8440 - Test Accuracy: 0.8105\n",
      "Test Loss: 43.6009 - Test Accuracy: 0.8103\n",
      "Test Loss: 43.7423 - Test Accuracy: 0.8119\n",
      "Test Loss: 44.1643 - Test Accuracy: 0.8133\n",
      "Test Loss: 44.3902 - Test Accuracy: 0.8148\n",
      "Test Loss: 45.6870 - Test Accuracy: 0.8129\n",
      "Test Loss: 46.4152 - Test Accuracy: 0.8111\n",
      "Test Loss: 46.7886 - Test Accuracy: 0.8125\n",
      "Test Loss: 47.7521 - Test Accuracy: 0.8108\n",
      "Test Loss: 48.0218 - Test Accuracy: 0.8121\n",
      "Test Loss: 48.1439 - Test Accuracy: 0.8134\n",
      "Test Loss: 48.4765 - Test Accuracy: 0.8147\n",
      "Test Loss: 49.0567 - Test Accuracy: 0.8159\n",
      "Test Loss: 49.1715 - Test Accuracy: 0.8171\n",
      "Test Loss: 50.7974 - Test Accuracy: 0.8141\n",
      "Test Loss: 51.4676 - Test Accuracy: 0.8139\n",
      "Test Loss: 51.6503 - Test Accuracy: 0.8151\n",
      "Test Loss: 52.7939 - Test Accuracy: 0.8135\n",
      "Test Loss: 52.8269 - Test Accuracy: 0.8160\n",
      "Test Loss: 53.1631 - Test Accuracy: 0.8171\n",
      "Test Loss: 53.2285 - Test Accuracy: 0.8195\n",
      "Test Loss: 54.0982 - Test Accuracy: 0.8179\n",
      "Test Loss: 54.5687 - Test Accuracy: 0.8177\n",
      "Test Loss: 55.5466 - Test Accuracy: 0.8175\n",
      "Test Loss: 56.7091 - Test Accuracy: 0.8160\n",
      "Test Loss: 57.8906 - Test Accuracy: 0.8146\n",
      "Test Loss: 58.3926 - Test Accuracy: 0.8157\n",
      "Test Loss: 58.4294 - Test Accuracy: 0.8179\n",
      "Test Loss: 58.5011 - Test Accuracy: 0.8200\n",
      "Test Loss: 59.6155 - Test Accuracy: 0.8186\n",
      "Test Loss: 60.4383 - Test Accuracy: 0.8172\n",
      "Test Loss: 60.6174 - Test Accuracy: 0.8182\n",
      "Test Loss: 62.0470 - Test Accuracy: 0.8157\n",
      "Test Loss: 62.0718 - Test Accuracy: 0.8178\n",
      "Test Loss: 62.3572 - Test Accuracy: 0.8176\n",
      "Test Loss: 63.3179 - Test Accuracy: 0.8163\n",
      "Test Loss: 64.3841 - Test Accuracy: 0.8151\n",
      "Test Loss: 65.0158 - Test Accuracy: 0.8149\n",
      "Test Loss: 65.8981 - Test Accuracy: 0.8147\n",
      "Test Loss: 66.2030 - Test Accuracy: 0.8156\n",
      "Test Loss: 66.5188 - Test Accuracy: 0.8165\n",
      "Test Loss: 66.7375 - Test Accuracy: 0.8173\n",
      "Test Loss: 67.8587 - Test Accuracy: 0.8152\n",
      "Test Loss: 68.0881 - Test Accuracy: 0.8160\n",
      "Test Loss: 68.1737 - Test Accuracy: 0.8178\n",
      "Test Loss: 69.0291 - Test Accuracy: 0.8176\n",
      "Test Loss: 69.0384 - Test Accuracy: 0.8194\n",
      "Test Loss: 69.2983 - Test Accuracy: 0.8202\n",
      "Test Loss: 69.4740 - Test Accuracy: 0.8210\n",
      "Test Loss: 69.6032 - Test Accuracy: 0.8226\n",
      "Test Loss: 70.2094 - Test Accuracy: 0.8224\n",
      "Test Loss: 70.3554 - Test Accuracy: 0.8231\n",
      "Test Loss: 71.1214 - Test Accuracy: 0.8229\n",
      "Test Loss: 71.4260 - Test Accuracy: 0.8227\n",
      "Test Loss: 72.4954 - Test Accuracy: 0.8216\n",
      "Test Loss: 72.8860 - Test Accuracy: 0.8223\n",
      "Test Loss: 73.6662 - Test Accuracy: 0.8221\n",
      "Test Loss: 73.7004 - Test Accuracy: 0.8237\n",
      "Test Loss: 73.7316 - Test Accuracy: 0.8252\n",
      "Test Loss: 74.8757 - Test Accuracy: 0.8241\n",
      "Test Loss: 76.0199 - Test Accuracy: 0.8239\n",
      "Test Loss: 76.4907 - Test Accuracy: 0.8246\n",
      "Test Loss: 77.4956 - Test Accuracy: 0.8244\n",
      "Test Loss: 78.0213 - Test Accuracy: 0.8250\n",
      "Test Loss: 79.1536 - Test Accuracy: 0.8248\n",
      "Test Loss: 79.3173 - Test Accuracy: 0.8254\n",
      "Test Loss: 79.8770 - Test Accuracy: 0.8260\n",
      "Test Loss: 79.8977 - Test Accuracy: 0.8274\n",
      "Test Loss: 80.0161 - Test Accuracy: 0.8288\n",
      "Test Loss: 80.5542 - Test Accuracy: 0.8294\n",
      "Test Loss: 81.8236 - Test Accuracy: 0.8283\n",
      "Test Loss: 82.0304 - Test Accuracy: 0.8289\n",
      "Test Loss: 82.2683 - Test Accuracy: 0.8295\n",
      "Test Loss: 82.5331 - Test Accuracy: 0.8300\n",
      "Test Loss: 82.6411 - Test Accuracy: 0.8305\n",
      "Test Loss: 82.6560 - Test Accuracy: 0.8318\n",
      "Test Loss: 83.8026 - Test Accuracy: 0.8316\n",
      "Test Loss: 85.3229 - Test Accuracy: 0.8306\n",
      "Test Loss: 85.8528 - Test Accuracy: 0.8304\n",
      "Test Loss: 86.5728 - Test Accuracy: 0.8301\n",
      "Test Loss: 87.2302 - Test Accuracy: 0.8292\n",
      "Test Loss: 87.7369 - Test Accuracy: 0.8297\n",
      "Test Loss: 88.3178 - Test Accuracy: 0.8295\n",
      "Test Loss: 89.4965 - Test Accuracy: 0.8279\n",
      "Test Loss: 89.5880 - Test Accuracy: 0.8291\n",
      "Test Loss: 90.6117 - Test Accuracy: 0.8275\n",
      "Test Loss: 91.6140 - Test Accuracy: 0.8270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.05206173, 0.93850005, 0.00943821],\n",
       "        [0.00123778, 0.00123432, 0.9975279 ],\n",
       "        [0.01509286, 0.20752849, 0.7773786 ],\n",
       "        ...,\n",
       "        [0.0068357 , 0.00525033, 0.987914  ],\n",
       "        [0.99210876, 0.00575821, 0.002133  ],\n",
       "        [0.8583273 , 0.13579574, 0.00587694]], dtype=float32),\n",
       " array([1, 2, 2, ..., 2, 0, 0]),\n",
       " 0.8064607187345204)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(softmax_prob_RoBerta,\n",
    " predictions_RoBerta,\n",
    " accuracy_RoBerta) = inference(RoBerta_model,\n",
    "                            X_test,\n",
    "                            y_test,\n",
    "                            tokenizer=RoBertaTokenizer)\n",
    "softmax_prob_RoBerta, predictions_RoBerta, accuracy_RoBerta\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Ensemble Model\n",
    "----\n",
    "\n",
    "Ensemble methods:\n",
    "\n",
    "To ensemble your two deep learning classifiers (Bert and RoBerta models), you can average their predictions. Here's a concise guide on how to implement this in PyTorch:\n",
    "1. Load Both Models: Ensure both models are loaded and set to evaluation mode.\n",
    "2. Prepare the Data: Tokenize and prepare your data as required by both models. Since you're using different tokenizers for Bert and RoBerta, you'll need to prepare the inputs separately for each model.\n",
    "3. Get Predictions from Both Models: Pass the test data through both models to get the logits (raw model outputs before applying softmax).\n",
    "4. Average the Logits: Average the logits obtained from both models. This simple method of ensembling is often quite effective.\n",
    "5. Apply Softmax and Determine Final Predictions: Apply softmax to the averaged logits to get probabilities, and then determine the final predictions (e.g., by taking the argmax).\n",
    "Here's how you could implement this:\n",
    "\n",
    "\n",
    "\n",
    "This code snippet averages the predictions from both models. You can also explore other ensembling techniques like weighted averaging or voting based on validation set performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_inference(model1, model2, X_test, y_test, tokenizers=[Bert_tokenize, RoBertaTokenizer], criterion=nn.CrossEntropyLoss()):\n",
    "    \"\"\"\n",
    "    Perform inference on the model\n",
    "\n",
    "    Return softmax probabilities, predictions, and metrics\n",
    "    \"\"\"    \n",
    "    # Assuming model_bert and model_roberta are your loaded models\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "\n",
    "    # Prepare data for both models\n",
    "    input_ids_bert, attention_masks_bert, _ = tokenizers[0](X_test, y_test)\n",
    "    input_ids_roberta, attention_masks_roberta, _ = tokenizers[1](X_test, y_test)\n",
    "\n",
    "    # Convert to appropriate device\n",
    "    input_ids_bert, attention_masks_bert = input_ids_bert.to(device), attention_masks_bert.to(device)\n",
    "    input_ids_roberta, attention_masks_roberta = input_ids_roberta.to(device), attention_masks_roberta.to(device)\n",
    "\n",
    "    # Get logits from both models\n",
    "    with torch.no_grad():\n",
    "        logits_model1 = model1(input_ids=input_ids_bert, attention_mask=attention_masks_bert)\n",
    "  \n",
    "        logits_model2 = model2(input_ids=input_ids_roberta, attention_mask=attention_masks_roberta)\n",
    "\n",
    "        # Average the logits\n",
    "        averaged_logits = (logits_model1 + logits_model2) / 2\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probabilities = torch.nn.functional.softmax(averaged_logits, dim=1)\n",
    "\n",
    "        # Get the predicted classes\n",
    "        _, predicted_classes = torch.max(probabilities, 1)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        acc = torch.sum(predicted_classes == torch.tensor(y_test['Sentiment'])) / len(y_test)\n",
    "\n",
    "        return probabilities.cpu().numpy(), predicted_classes.cpu().numpy(), acc.item()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.14485256, 0.7910762 , 0.06407132],\n",
       "        [0.0308138 , 0.04160151, 0.92758477],\n",
       "        [0.07597816, 0.35849625, 0.56552565],\n",
       "        ...,\n",
       "        [0.06800987, 0.07859176, 0.8533984 ],\n",
       "        [0.8673193 , 0.09128197, 0.04139874],\n",
       "        [0.6086164 , 0.33969036, 0.05169322]], dtype=float32),\n",
       " array([1, 2, 2, ..., 2, 0, 0]),\n",
       " 0.8192686438560486)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensemble Inference\n",
    "(softmax_prob_ensemble,\n",
    " predictions_ensemble,\n",
    " accuracy_ensemble) = ensemble_inference(Bert_model,\n",
    "                                        RoBerta_model,\n",
    "                                        X_test,\n",
    "                                        y_test,\n",
    "                                        tokenizers=[Bert_tokenize, RoBertaTokenizer])\n",
    "\n",
    "\n",
    "(softmax_prob_ensemble,\n",
    " predictions_ensemble,\n",
    " accuracy_ensemble) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1422, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute evaluation metrics\n",
    "def eval_metrics(predictions, y_test):\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions, average='weighted')\n",
    "    recall = recall_score(y_test, predictions, average='weighted')\n",
    "    f1 = f1_score(y_test, predictions, average='weighted')\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, predictions)\n",
    "    return accuracy, precision, recall, f1, conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1, conf_matrix = eval_metrics(predictions_ensemble, y_test['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:   0.8193\n",
      "Precision:  0.8184\n",
      "Recall:     0.8193\n",
      "F1 Score:   0.8180\n",
      "Confusion Matrix: \n",
      "[[360  82  24]\n",
      " [ 68 392  49]\n",
      " [  7  27 413]]\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy:   {accuracy:.4f}')\n",
    "print(f'Precision:  {precision:.4f}')\n",
    "print(f'Recall:     {recall:.4f}')\n",
    "print(f'F1 Score:   {f1:.4f}')\n",
    "print(f'Confusion Matrix: \\n{conf_matrix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.77      0.80       466\n",
      "           1       0.78      0.77      0.78       509\n",
      "           2       0.85      0.92      0.89       447\n",
      "\n",
      "    accuracy                           0.82      1422\n",
      "   macro avg       0.82      0.82      0.82      1422\n",
      "weighted avg       0.82      0.82      0.82      1422\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArcAAAIxCAYAAABXdJCyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/SrBM8AAAACXBIWXMAAA9hAAAPYQGoP6dpAABVUklEQVR4nO3ddXxX1R/H8fd3YwEDRgvbaNjIUdLdHUPpkhBQUUFSBQXpkEZKQkAapBnd0s3YAOnu2Fiw+P7+QL7+5gaMsby+no+HD9055977uV8cvDk791yT2Ww2CwAAADAAq/guAAAAAIgphFsAAAAYBuEWAAAAhkG4BQAAgGEQbgEAAGAYhFsAAAAYBuEWAAAAhkG4BQAAgGEQbgEAAGAYhFsAiZqXl5d++OEH1apVS4UKFVLRokXVvHlz/f777woJCYmzOkJCQjRy5EiVLVtWBQsWVP369WPlOjdu3JCbm5s+//zzWDl/VLRp00Zubm5yc3PTkSNH3ji2fv36cnNzU5UqVaJ9vaCgIM2ePTvK493c3NSwYcNoXw9A4pYkvgsAgOgICwvTpEmTNHXqVNnY2KhChQqqXLmyfH19tXfvXv3000/y9PTUzJkzZW9vH+v1LF++XLNnz1b27Nnl4eGhtGnTxsp1UqZMqW7duilHjhyxcv53tWXLFn344YeR9l25ckXnz59/72u0bt1aly9fVocOHaI0vlu3bkqXLt17XxdA4kS4BZAoTZs2Tb/88osKFy6siRMn6oMPPrD0vXjxQt99953Wrl2rfv36afz48bFez9mzZyVJP/zwg8qUKRNr10mZMqW+/PLLWDv/u0ifPr22bNmib7/9NtJ+T09P2djYyGQyvdd1Hj58+E7jE8rnAyB+sCwBQKJz+fJl/fLLL0qTJo1mzpwZLthKkq2trYYPHy5nZ2d5enrq4sWLsV7TixcvJEmpU6eO9WslFFWrVtXNmzctwf7fNm3apNKlS8vOzi6OKwPwX0a4BZDorFq1SsHBwWrVqpVSpkwZ6RgbGxsNGDBAw4YNixA4N2zYoObNm6tw4cIqUqSImjdvrvXr10c4h5ubm/r166djx46pTZs2KlKkiIoXL67u3bvrxo0bkv5ZA/vHH39Ikho1aiQ3NzcdPHhQK1eulJubm+bOnRvh3K/WrT579szSdvr0aXXp0kXlypVTwYIFVbNmTY0ZM0Z+fn6WMa9bc3vv3j398MMPqlixogoUKKCKFSvqhx9+0L1798KNmzRpktzc3HTx4kWNHTtWlSpVUoECBVS3bl0tWrToDZ96RDVr1pQkbd68OULf9evXdfbsWcuYf3v+/LmmTJmihg0bqkiRIipYsKBq1KihUaNGyd/fP9y93rx5U76+vpZfj1efX5UqVbRr1y5VqVJFhQoV0tdffy0p/JrbK1euqFChQipSpIju3r0broaOHTvKzc1Na9aseaf7BpCwEW4BJDp79uyRJJUvX/6N4ypXrqzGjRsrTZo0lraRI0eqR48eunHjhurVq6e6devqxo0b+uabbzR69OgI5/Dy8lLbtm1lZWWlFi1ayM3NTRs3btQnn3yiFy9eWNbA5smTR5LUrFkzdevWTc7Ozu90T5cvX1b79u11/PhxValSRe3atVO6dOk0c+ZMffHFF2889tq1a/Lw8NCSJUuUI0cOtW7dWjly5NCSJUvUuHFjXb9+PcIxvXv31tKlS1WhQgU1bdpUd+/e1cCBA7V06dIo11ygQAE5Oztry5YtEfo8PT2VJEkSVatWLUJfSEiI2rdvr0mTJil9+vRq2bKlPvroIwUGBmrWrFmWAPvqs02RIoVsbW3VrVu3cOd7/PixunfvrqJFi8rDwyPStb/ZsmVT9+7d5e/vryFDhljaFy9erL1796p27dpq0KBBlO8ZQMLHmlsAic6dO3ckvQwu7+LIkSOaPXu28uXLp1mzZllC76NHj9SuXTv9+uuvqlSpkooXL2455vz58+rdu7c6deokSTKbzerUqZP27t2rAwcOqEKFCvryyy918+ZN+fj4qEWLFsqbN68k6dChQ1GubenSpfL19dVvv/2mUqVKWdq7dOminTt36sKFC8qdO3ekxw4YMEAPHjzQkCFD1KRJE0v7woULNWjQIPXv31+//fZbuGOePHmiDRs2WD6DevXqqUWLFlq+fLmaNm0a5bqrV6+uuXPn6vLly8qePbulfdOmTSpVqpRSpUoV4ZhNmzbp5MmT6tq1q3r06GFp79Wrl2rWrKmtW7cqICDAsr74jz/+0LNnzyKspfX391f79u0tYfh12rVrp02bNmnz5s3atWuXcubMqZEjRyp9+vQaOHBglO8VQOLAzC2AROfVj/IdHBze6biVK1dKkvr06RNuNjdNmjTq2bOnJGnFihXhjrG3t1fbtm0tX5tMJsuM8c2bN9+9+NcICwuT9HJpwv8bPny49u/f/9pge/v2bR04cEAffvhhuGArSS1btlTBggV14MAByzKKVz766KNwn0HRokWVMmXKd76nGjVqSFK42dtbt27p9OnTqlWrVqTH5MuXT0OGDFG7du3CtSdPnlz58uVTaGionj59+k7XfxMrKysNGzZMdnZ2GjZsmPr37y9/f38NGzYs0vANIHEj3AJIdF4Fkv9frxoVPj4+srKyUrFixSL0vWrz8fEJ1+7k5CRbW9twbSlSpJD0z0NkMcHDw0N2dnYaM2aMKlasqAEDBmjLli2yt7cPF0L/zdvbW5Jeux1X0aJFJUW8r/+fZX0lefLk73xPRYsWVfr06cOtu920adNrlyS8unaTJk3k4OCgkydPatWqVZo4caK6dOlime0ODQ2N0vVdXFyiNC5Hjhz66quvdOXKFe3fv18tWrRQhQoVonQsgMSFcAsg0cmcObMk6erVq28c5+vrG+6BKj8/P9nZ2UUIq9LLwJo0aVIFBASEa49s7Kutrcxm8zvX/jp58uTR0qVLVbt2bT179kxLly5Vt27dVLZsWY0bN+6113r1sNmrwP1vGTJkkCQFBgaGa3/dfb3rPZlMJlWvXl1nzpyxLBfZtGmTSpYs+dqdI8LCwjR16lSVL19eTZs2Vd++fbV48WIlSZLEslY5qnW8yx7G1atXt/zaFSlSJMrHAUhcCLcAEp1XywL27dv3xnFLlixR+fLlLfvcOjg4KCAgINIZ36CgIAUGBsboVl5vCsH/DtHSy4A7fvx4HTx4UPPmzVOnTp1kb2+vadOmvXYng1dLM/69E8Arr+41Nn/8XqNGDZnNZm3ZskV3797ViRMnXrskQZJmz56t8ePHy83NTTNnztTevXv1559/asqUKXJycoqVGs1mswYMGCDp5YNqw4cP16NHj2LlWgDiF+EWQKJTv3592djYaMGCBfL19Y10TEBAgJYtWyZJKlu2rCRZdjQ4evRohPFHjx6V2WxWrly5YqxOGxsbSbJsbfWK2WyOsIPBqlWrNHjwYJnNZtna2qpkyZLq3bu3Jk2a9NqaJVkeXjt27Fik/YcPH5bJZIrR+/q3EiVKKHXq1NqyZYu2bNkiKyur1y5JkKR169bJ2tpaU6dOVYUKFZQ+fXpJLz+XS5cuWf47Ji1cuFAHDx5U06ZN1b9/fz1+/FiDBg2K0WsASBgItwASncyZM+uTTz7R48eP1alTpwh7ufr6+qpXr166cuWKKleubNn9oHHjxpKksWPHhpu1e/TokUaNGiVJlv1RY8KrV+Tu2bMn3BrShQsX6smTJ+HGnjhxQgsWLNDGjRvDtb96EOx1M5pOTk4qWbKkzpw5o4ULF4brW7ZsmY4dO6aSJUsqY8aM73s7r2Vtba2qVavq6NGjWrlypUqWLPnGdcJ2dnYKDQ2NMHM6ZcoUywNtISEhlnYbG5twX7+rGzduaMyYMUqfPr169eqlhg0bqnTp0vL09NSmTZuifV4ACRNbgQFIlHr06KGHDx9q5cqVqlq1qipVqqQsWbLo7t272rdvnx49eqSiRYtaQqskFS9eXO3bt9ecOXPUoEEDVa5cWZK0Y8cO3b9/X59++mm4bcDeV758+ZQ/f34dP35cLVu2VPHixXXu3DkdOHBAhQoV0smTJy1jO3XqpI0bN6pXr17y9PRU1qxZdfPmTW3evFnp06dX69atX3udn376Sa1atdKgQYO0ZcsWubm56fz589q3b58yZMigwYMHx9g9vU6NGjW0fPlyeXl56aeffnrj2AYNGujEiRNq0aKFateuLRsbGx08eFBeXl5KmzatHj58GC78Z8iQQVeuXFGvXr1Urlw5NWrUKMp1mc1my+4IQ4YMsbz0Y+DAgWrQoIEGDRpkmXkGYAzM3AJIlKytrTV8+HDNmjVLFStWlI+Pj+bPn6/t27crW7ZsGjRokBYsWBDhDWb9+vXT6NGj5ezsrLVr12rjxo3Knj27Jk2apF69esV4ndOnT5eHh4euXLmiBQsWKCAgQL/99psKFSoUbpyLi4sWLVqkOnXq6MyZM5ozZ44OHz6sBg0aaOnSpRFeMfz/smXLphUrVqhp06b666+/tGDBAl25ckVt2rTRqlWrlCVLlhi/r38rXbq0UqZMKWtra1WvXv2NY1u2bKkBAwYoVapUWrZsmdauXSsHBweNHTvWEox37dplGd+7d2/lzp1bnp6eWr169TvVtWTJEu3fv1/ly5dX3bp1Le3ZsmVT165d9fDhwzgJ/wDijskc0wubAAAAgHjCzC0AAAAMg3ALAAAAwyDcAgAAwDAItwAAADAMwi0AAAAMg3ALAAAAwyDcAgAAwDB4Q9nfkn00O75LABKdQxOaxncJQKLkkjZpfJcAJEqpklq/dQwztwAAADAMwi0AAAAMg3ALAAAAwyDcAgAAwDAItwAAADAMwi0AAAAMg3ALAAAAwyDcAgAAwDAItwAAADAMwi0AAAAMg3ALAAAAwyDcAgAAwDAItwAAADAMwi0AAAAMg3ALAAAAwyDcAgAAwDAItwAAADAMwi0AAAAMg3ALAAAAwyDcAgAAwDAItwAAADAMwi0AAAAMg3ALAAAAwyDcAgAAwDAItwAAADAMwi0AAAAMg3ALAAAAwyDcAgAAwDAItwAAADAMwi0AAAAMg3ALAAAAwyDcAgAAwDAItwAAADAMwi0AAAAMg3ALAAAAwyDcAgAAwDAItwAAADAMwi0AAAAMg3ALAAAAwyDcAgAAwDAItwAAADAMwi0AAAAMg3ALAAAAwyDcAgAAwDAItwAAADAMwi0AAAAMg3ALAAAAwyDcAgAAwDAItwAAADAMwi0AAAAMg3ALAAAAwyDcAgAAwDAItwAAADAMwi0AAAAMg3ALAAAAwyDcAgAAwDAItwAAADAMwi0AAAAMg3ALAAAAwyDcAgAAwDAItwAAADAMwi0AAAAMg3ALAAAAwyDcAgAAwDAItwAAADAMwi0AAAAMg3ALAAAAw0gS3wXgvyNNcjt917SwahXLrEypk+nKPV8t2PGXJq49o9Awc7ix1Qs7q6eHu4rkTKeg4FAdv/hAPy06pqMXH0Q4b62iLur7cWHly5JaAS9CtPHIdf2w4IjuPwuMq1sD4pzv0ydaOOcXHflzt549fazUadOrTMXqataus+zsk1rGPX70QEt/m6GjB/fq6eOHSp7CUe5FS6jZJ12V0cklHu8AiB8PH9zXzGlTtG/Pbj16+EApHR1VomRpdf78Szm7ZH7tccsW/64xI4ZqwKChqtfQIw4rxrti5hZxIrl9Em0dWlef180v7+tPNM3TW8/8gzW0bXEt6Vs13Nj21Vy1ekBN5cqUUr9tO68Nh6+pXL6M2jq0rorlTBdubJNyObTy+xpK72ivmZu8tev0bbWulEvbh9WTYzLbuLxFIM4EBPjr++4dtXntCjllzqo6jVsoTdr0Wr10ngb1+UKhoSGSXgbbfl+01eZ1K+SSJZvqNG6hXHnya892T/X9vK1u3bgWz3cCxK2HD+6rfetm+mP5UmXLnkPNWrZR/gLu2rRxvT5p1VTXrl6J9Ljbt27ql4nj4rZYRBszt4gTvRsXUh6XVOo564CmbjhraZ/TvaKalc+pWkVd5HnshlzSOWh0h1Lyvv5YNQZs0EPfIEnSrC3ntH1oXQ1u86HqDPSUJDnYJ9G4TqV16c4zle61Wr4BwZKkbSdza9oX5dX340L6bt7huL9ZIJZtWbtCN69dUd3GLdThi16SJLPZrInDB2j3to3avXWjKtesr6W/zdCDe3fVrmsPNWjS2nL8ri0bNHHEAP02bZy+HcIf2PjvmDltiu7euaOve/ZRyzafWNo3rl+jgd/308SxozVmwpQIxw0fPFD+/v5xWCneBzO3iBNZMiTX9ft+muHpHa59+d5LkqSSbhkkSZ9UdVUyuyTqNfugJdhK0uEL9zV29WmduvLI0ta0XA6lSWGnyeu8LMFWkuZtv6BzN5+odeXcsrIyxeZtAfHir3Mv/4JYpVZDS5vJZFLVOo0kSee9T0uSDu7doZSpUqveRy3DHV+xeh1ldHLRiSP7FRYWFjdFAwnAzu3blDp1GjVv1TZce+26DeSSObMO/Lk3wvfE2lUrdXD/PpUpVyEuS8V7YOYWcaL9+F2Rtrs6p5Ik3X0SIEmqUcRFj3yDtPP0rQhjf/z9aLivy+bLKEnadeZ2hLF7ztxRp5p5lD9zap2++ihCP5CYpUjpKEm6f/e2suXMbWl/9OCeJCmlY2qFhoaqccv2SpIkiaysIs5jJLGxVUhwsEJDQmRlyxIeGF9oaKg+6dj5td8TNja2Cg4OVkhIiGz//p54cP++Jvw8SnXrN1Jutzz6c+/uuC4b0ZBgw+2DBw9069YtBQQEyGQyKXny5Prggw+UNm3a+C4NMSB9Snt5lM6m/s2K6Np9Py3efVGSlMcllc5cfaSMqZJqUKsPVbOoi5LZJdGfPnc1YP6RcDO3OT5IIUm6fNc3wvmv3veTJOVySkm4heFUqd1QWzeu0typPytFypTKnstNF3y8NH/mJCVzSK6qtRvK2to6woztKzeuXdat61eU0clFNgRb/EdYW1ureas2kfZduXxJV69clkvmzJZgK0mjhv2kJDY2+rpXH21YuyauSsV7SlDh9smTJ5oxY4bWr1+ve/fuRTomY8aMql+/vtq3b6/UqVPHcYWICT80L6p+TQpLku4+9lf9nzz15PkLOSazVfKkNrK3tdbukQ30PDBES/deUsbUydSwZFZtHVJXtX7coGMXH0qS0qSwV+CLEAW+CI1wjWf+LySJh8pgSDld8+qHUb9o/JDv9P3XHS3t6TJk1NCJs5Uho9Nrjw0LC9Ovk0YpLCxM1es2jotygQQtLCxMY0YMUVhYmBo1bmpp37Jpo3bt2KYhI3+Wo2Oq+CsQ7yzBhNsbN26odevWun//vkqVKqWGDRsqQ4YMsrOzkyQFBQXp3r178vLy0qxZs7R+/XrNmzdPzs7O8Vw53tXlu74a88cp5c6UUvWKZ9GWIXXVaPBm3Xv6cmlC4RzptP3ULX08fIsluNb9MLOWfVtdk7uWVZneL//2bJPESkHBka8XDAp+eZy9rXUc3BEQt54+fqSFsybr8aMH+rB0BTm5ZNHF897yOnlU08cN1XdDJ8gheYoIx5nNZk0fN1Snjx1STrd8qvuamV3gv8JsNmvE4IE6fPCA8uYroOatX87sPnn8WD+PGKryFSures3a8Vwl3lWCCbcjRoxQSEiIVq1apdy5c79x7Pnz59WhQweNGjVKEyZMiKMKEVPm77hg+e/axTJrWb9qmvlVBdUf5Glp/3buoXAzsuuPXNeuM7dVsUAm5cyUUhdvP1PAixDZJkmqyNjZvAy1zwNDYukugPgzbtj38jlzUt8MGK6ylWpY2tcu/11zp47V1LFD1OuHkeGOCQ0N0dSfh2jHprX6IJOz+v00VjY2NnFdOpBghISEaNhPP2j9mlVydsms0eMny8bm5U/7xo4apqAXQer93YB4rhLRkWB2Szhw4IA6duz41mArSa6urmrfvr0OHToUB5UhNm08el07Tt9S/iyplc7RXpL0IjhUZ65FXCd76vLL5Qiv1to+8XuhpHZJZJsk4v/GKf9ejvBqeQJgFA/v39XpY4eUz71ouGArSfU/biWXrDl0cM92Bfg/t7QHBQZoxIBvtGPTWmVyzqJBP09XmnTp47p0IMEIDAhQ7+7dtH7NKmXOklW/zJyj9Ble7tqzd/dObdq4Xl989Y0++CBjPFeK6Egw4dbGxkbBwcFvH/g3s9msFy8ILomBtZVJld2dVMU98nWA1/9++CupbRLdevhc1lYmWUeyhZfN3yHW/8XL2di/bj2VJGXNkDzC2Gx/t53/ewxgFA/u3ZUkuWTJHml/5qzZFRYWpod/75zg5/tMP/bqqmMH9yl7LjcNnTBL6T/IFGf1AgnNs2dP9Xnn9vpz72655cmrGXMXKGOmf/582r5lsyRp9PDBKlk4n+Wf8WNGSJIG//i9ShbOp6OHmWBLqBLMsoRSpUpp7ty5Kl++vPLmzfvGsd7e3po1a5ZKly4dR9XhfS3vV02+gcHK0Wmxwv71qt2C2dIoLMysq/d8tc/7rpqUy6Hy+TNpx6nw24EVyZFOwSFh8rn+RJL0p89dta3qqvL5M+nCrWfhxpbPn0lPngfJ58aT2LwtIM45pk4jSbp142qk/bdvXpfJZJJjqjR68SJIw77vrgveZ5S/UDH1GzxWyRwi/mUQ+K8ICgpSzy8/l9fpUyparLhGT5ii5MnDf09UrFxFmZwiTsacOX1KB/7cqwqVqsjVLY8yOfHMT0KVYMJtnz591KZNG3300UcqVKiQChQooIwZM8re3l4mk0mBgYG6f/++zpw5o2PHjilt2rTq169ffJeNKAgNM2v1watqXiGnejQsqJ//OGXp+7RmHhXLlV4bjlzTvaeBmr3lnJqUy6GhbT5UjQEb5Pf3mtmPymRXSbcMWn3giuXlDmsPXdWo9iXVo2FB/bH/sh77vZzJb1slt1ydHTV+9WmZzRHrARKzjE4uyumaV14nj+rQvp0qUbaSpW/rhlW6cvG8ihQvoxQpHTXnl7E653VSbvnc9f3wibKzs4+/woEEYOqk8Tp18rgKuhfWuCnTZW8f8XuiYpVqqlilWoT2RQvm6cCfe1WxclXVa+gRF+Uimkxmc8L54//p06eaM2eO1q9fr+vXr0c6JkuWLKpVq5Y6duwoR0fHGLt2so9mx9i5EJFTmmTaObyeXNIl15bjN3Tm2mMVyp5WVdyddPmur6p9v063H7/cLWF0h5L6om5+Xbvvp1UHrsg5rYMalcyqB88CVeHbtbp+/5+1hJ1quGlil7K6ft9PK/68LKe0DvqodDZdvOOryt+ttQRexI5DE5q+fRBi3JWL5/XDN50V4P9cxUqVl3PmbLp66YKOH/5TqdOm09AJs2VrZ6euLespJDhYVWo1VLoMH0R6Lo8Wn8jW1i6O7wAuaSN/GBax5+GD+2pYu5qCg4NVv1Hj166nbdvhU8tOTf9v0YJ5Gj9mhAYMGkq4jUepkr59F6QEFW7/37Nnz3T37l09f/5cZrNZyZIlk7Ozc4QfH8QUwm3s+yBVUg1oXlS1i2VWupT2uv3YX6sPXNHI5Sf1yC8o3NjWlXOpa618yps5lXwDg7X1xE0NWnQ0XLB95aMy2fVNo4LK45JKj/2CtPXETQ1ceFR3/n7rGWIP4Tb+3Ll1XcvmzdSJowfk+/SJHFOnVbGS5dSsXWelTpteB/fu0Kgfe731PPNW74x02zDELsJt3Nu1fav6fPPVW8dt3X1AKVKmjNBOuE0YEnW4jWuEW+DdEW6B6CHcAtETlXCbYHZLAAAAAN4X4RYAAACGQbgFAACAYRBuAQAAYBiEWwAAABgG4RYAAACGQbgFAACAYRBuAQAAYBiEWwAAABgG4RYAAACGQbgFAACAYRBuAQAAYBiEWwAAABgG4RYAAACGQbgFAACAYRBuAQAAYBiEWwAAABgG4RYAAACGQbgFAACAYRBuAQAAYBiEWwAAABgG4RYAAACGQbgFAACAYRBuAQAAYBiEWwAAABgG4RYAAACGQbgFAACAYRBuAQAAYBiEWwAAABgG4RYAAACGQbgFAACAYRBuAQAAYBiEWwAAABgG4RYAAACGQbgFAACAYRBuAQAAYBiEWwAAABgG4RYAAACGQbgFAACAYRBuAQAAYBiEWwAAABgG4RYAAACGQbgFAACAYRBuAQAAYBiEWwAAABgG4RYAAACGQbgFAACAYRBuAQAAYBiEWwAAABgG4RYAAACGQbgFAACAYRBuAQAAYBiEWwAAABgG4RYAAACGQbgFAACAYRBuAQAAYBiEWwAAABgG4RYAAACGQbgFAACAYRBuAQAAYBiEWwAAABgG4RYAAACGQbgFAACAYRBuAQAAYBiEWwAAABgG4RYAAACGQbgFAACAYRBuAQAAYBiEWwAAABgG4RYAAACGYTKbzeb4LiIhOH/XP75LABKdQk1GxXcJQKJ0YfW38V0CkCi5pLZ76xhmbgEAAGAYhFsAAAAYBuEWAAAAhkG4BQAAgGEQbgEAAGAYhFsAAAAYBuEWAAAAhkG4BQAAgGEQbgEAAGAYhFsAAAAYBuEWAAAAhkG4BQAAgGEQbgEAAGAYhFsAAAAYBuEWAAAAhkG4BQAAgGEQbgEAAGAYhFsAAAAYBuEWAAAAhkG4BQAAgGEQbgEAAGAYhFsAAAAYBuEWAAAAhkG4BQAAgGEQbgEAAGAYMR5uQ0JCYvqUAAAAQJREO9xeuXJFo0ePltlsliTdvHlTzZo1U8GCBVWhQgX98ccfMVYkAAAAEBXRCrdnz56Vh4eHZs+erVu3bkmSBgwYoJMnTypLliwKDg7Wd999p127dsVosQAAAMCbRCvcTp06VWFhYRo/frwyZcqkmzdv6s8//1ThwoXl6ekpT09PZcyYUXPmzInpegEAAIDXila4PXr0qOrUqaOaNWvKyspKO3bskCTVr19fJpNJjo6Oqlq1qs6cOROjxQIAAABvEq1w6+fnp/Tp01u+3r17t0wmk8qVK2dps7GxsazHBQAAAOJCtMKts7OzLly4IOll0D106JAyZ86srFmzWsYcOHBAzs7OMVMlAAAAEAXRCrfly5fXzp079e2336pTp04KCgpS/fr1JUknT55U165d5ePjY2kDAAAA4kKS6BzUvXt3Xb582bLdV+HChdWpUydJ0qZNm7Rz507VrFlT7dq1i7lKAQAAgLcwmd9jYeyFCxcUFhYmNzc3S5uPj49evHghd3f3GCkwrpy/6x/fJQCJTqEmo+K7BCBRurD62/guAUiUXFLbvXVMtGZuX8mdO3eEtjx58rzPKQEAAIBoi1K4nTdvXrQv0LZt22gfCwAAALyLKIXbYcOGyWQyvfPWXiaTiXALAACAOBOlcDt8+PDYrgMAAAB4b1EKtx4eHrFdBwAAAPDe3uuBspCQEO3bt08+Pj56+vSp+vTpo3PnzsnBwUEuLi4xVSMAAAAQJdF6iYMkHTx4UNWqVVPXrl01btw4zZkzR5K0ceNG1axZU7NmzYqxIgEAAICoiFa49fb2VufOnRUQEKAuXbqoRo0alr7ChQsrXbp0GjNmjLZv3x5jhQIAAABvE61wO3HiRNnZ2WnlypXq3r27XF1dLX2VKlXSsmXL5OjoaJnNBQAAAOJCtMLt0aNHVatWLTk7O0fanyFDBtWuXVsXLlx4r+IAAACAdxGtcBsUFKRkyZK9cYy1tbWCgoKiVRQAAAAQHdEKtzlz5tS+ffsUFhYWaX9wcLD27t2r7Nmzv1dxAAAAwLuIVrht0qSJLly4oH79+unx48fh+h4+fKhevXrp6tWraty4cYwUCQAAAERFtPa5bdGihY4fP641a9Zo7dq1srOzkyRVqVJFd+7cUVhYmKpVq6ZWrVrFaLEAAADAm0T7JQ6jRo1S5cqVtXz5cp09e1YhISHy8/NTsWLF5OHhwawtAAAA4tx7vaGsdu3aql27dkzVAgAAALyX9wq3knTr1i35+PjI399fjo6Oypcvn9KmTRsTtQEAAADvJNrh9uzZsxoyZIiOHz8ert1kMqlixYoaMGCAnJyc3rtAAAAAIKqiFW59fHzUqlUrBQYGqmzZsnJ3d5eDg4Pu3bun48ePa8eOHfLy8tLSpUuVMWPGmK4ZAAAAiFS0wu348eMVHBysGTNmqHz58hH616xZo759++rnn3/W6NGj37tIAAAAICqitc/t4cOHVbNmzUiDrSQ1aNBAVatW1e7du9+rOAAAAOBdRCvcWllZKUOGDG8ckyVLFoWEhESrKAAAACA6ohVuq1Wrpo0bN8rX1zfS/qCgIG3fvv21M7sAAABAbIjSmlsfH59wXzds2FB79uzRxx9/rM8//1xFixZV2rRp5evrq9OnT2v69Okym836+uuvY6VoAAAAIDIms9lsftugPHnyyGQyhWt7ddi/2/+/z8rKSmfPno2JOmPd+bv+8V0CkOgUajIqvksAEqULq7+N7xKARMkltd1bx0Rp5rZRo0aRhlgAAAAgIYlSuB0xYkRs1wEAAAC8t2g9UBZV169fj83TAwAAAOFE+/W7u3bt0tq1a/Xo0SOFhoZa1tmazWaFhIToyZMnunLliry9vWOsWAAAAOBNohVuN2/erK+//lpvehYtadKkqlq1arQLAwAAAN5VtJYlzJkzR9bW1ho/frz27dunfPnyqWnTptq3b59+++035c+fXyaTSb169YrpegEAAIDXila4PX/+vKpVq6ZatWopbdq0Klq0qI4ePaq0adOqZMmSmjVrlmxtbTVt2rSYrhcAAAB4rWgtSwgKClLWrFktX+fIkUOLFi3SixcvZGtrq1SpUqlatWo6cuRIjBUKY9u5eYPWLF+oq5cvysEhufIWLKS2nb+Uc+Z//j8LDAzQ4rkztGf7Zj16cE8pHVOpeJmKavPpF3JMlToeqwdiV5qUSfX9J5VUq3RuZUqXQlduP9GCjSc0Yel+hYaGWcY5JLVVv7YV1KRKAWVI46Brd55qgecJTVp2QEEvwr8O3SaJtbo3L6MWNdyVPVNqBQWH6Ij3TY2cv0d7TlyJ4zsE4s+0iWO0bOE8/TxllgoXK25pD/D314I5M7Rji6ceP36oDzJmUo06DfRx8zaytXv7XquIP9GauU2XLp0ePXpk+TpLliwKCwvThQsXLG2pU6fW3bt3379CGN78mVP085Dv9dzPV3UaNVHBIsV0YO9O9eraVndv35IkhYWFaWDvblqxcK5Spkqleh+1UNYcubVp7Qr1+fwTPfeL/FXQQGKXPKmttk3poM8/LqmzV+5r2spDeuYXqKGfVdeSIc0s45La2WjT+Hbq1aqc/AJe6NfVR/TXjYca3KWa1oxuLXvbf+YyTCaTVoxooZ86V1VIaJhmrj6s1bu9VapAZm0c11aNK+WLj1sF4pyP12mtWPJ7hPbAwAD1/KKjFs+fraTJkqq+RxM5u2TRrKkT1a/7ZwoKDIyHahFV0Zq5LV68uDZv3qwOHTooe/bsypMnjyRp27Ztyp8/vyTp2LFjcnR0jLlKYUjnz57RsgWzVKBwMQ0cPVl2dvaSpDIVt2jED320+LcZ+rrfQO3fvV1eJ4+pdPkq6jd4tKysXv69bN6MSVq2YLbWLFuoFu27xOetALGid+vyypM1vXpO2KhfVhy0tM8d8JGaVS+oWqVyy/PABX3TsqyK5XXW6t3eajNwuYJDQiVJnRsV14Rv6qpnq3IaOmenJOnjyvlVvUQurdp1Vq0HLrfM/v68cJ/2zvhU43rU1bp95/QiODTO7xeIK8HBwRo99AeFhUb8/3zJ/Dk65+2lchWrqv+QUbKxsZEkrV6+WBPHDNPi+bPV7tPP47pkRFG0Zm47d+6swMBA1a9fX56enkqXLp0qV66s6dOnq3v37mrTpo2OHTumMmXKxHS9MJj1fyyRJHXrPcASbCWpTMVqqln/I2V0cpEkXfDxkiRVrV3fEmwlqWb9jyRJPmdPxVXJQJzKmjGVrt99qumrDodrX7b9jCSpZIHMkqQmVQooLMysHuM2WIKtJM1YdVjnrz3QZ41LyNr65fdOwwp5JUmDZ+8Mt6zh/LUHWr7dSxlSO6iYm1Os3hcQ336fO0M3r19T0eKlIvTt2Oopk8mkL3t9awm2ktTgo2ZyyZJVfyxbpNCQkAjHIWGI1sxt7ty5NX/+fE2cOFEpUqSQJA0YMEDXr1+Xp6enJMnd3V09e/aMuUphSEcP7lPWHLnCra2VXv7YtFvv/pavUzqmkiTdu3s73LiHD+5JEmtuYVifDF4RabtblnSSpHuP/CRJ2TK9DMG3H0ZcouN16Z48KuVTnqzp5HXpnlbs8NL56w90/tqDCGODgl/+ge2Q1DambgFIcC5eOK9Fv81Sy3ad5Ofnq2OHD4Trv3PrpjJ8kEnp0mcI124ymZQ9Z27t2bFVV69cUo5crnFZNqIo2i9xcHd316+//mr5OlOmTFq7dq18fHxkZ2enbNmyyWQyxUiRMKYnjx/p6ZPHKvRhSV2/elnzZ0zWqeOHZDZLRYqX0idduyujk7MkqULVWlo6/1ctnjtDmZxcVKDwh7px7bKmjBmiJDY2quvR7C1XA4whfSoHeVTKp/4dKunanSdatPnlTy2CgkNla2sd6TEpHV4+/JLlg1TyunRPf+w6qz92nY0wziaJtWqVyi1J8rl6P5buAIhfoaGh+nnYj3LOnFUtP/lUMyaPjTDGxsZWwcEvIj3+ud/Lv1DevXObcJtARTvcvs6r9bfHjh3T48ePeZEDXuvRg5d/eD68f089u7RRJufMqlanoW5cvaJ9O7fK6+Qx/Tx9vjJkdFK6DB9o+KRZGj3oWw3q+5XlHMlTpNSQsdPklq9gfN0GEGd+6FhZ37arKEm689BP9XrO1xO/lw+2HDt3S5WKZlfJ/C466HXDckz6VA4qnu/l8h7H5G9+wrt363LKlim1PA9c0I17z2LpLoD4tfT333ThnLfGT/8t3JKD/+eaN59OHD0sr9Mnlb9gIUv740cP5eN1WtI/IRcJT7TW3EbFuHHj1K1bt9g6PQwgMDBAkv5+UKyyxs5YoE7demng6Mnq/HUfPXn8SDMnjXk5NiBAC2dP0/Url+RepLgaNWuj4mUq6Lmfr6aMGRJhuQJgRJdvPdaY3/dq9W5vpU+VTFsnd1Bh10ySpIlL9kuS5g9soholc8khqa3cc2XUkqHNZGX5Kdrrf5rWsmYhff9JJT3xDVT3cetj+1aAeHH92hXNmzVVDRo3Cxda/+3jFm0lSUP699bBP/cowN9ff5330Y99eyjM/HKd+pve0or4FWvhFnibV3/gWllbq9OXvWRt/c+PVOt6NFNGJxcd2b9HgYEBmjFxlA7s2aFPun6toRNmqOMX3+iHERPU76fRun71skb+0Ce+bgOIM/M3ntCA6VvVvP8SNflusdI5JtOv33lIkjbuP69vf9msjGmTa/Xo1nqw6TsdnN1V/oHBGr/kT0lSQFBwpOdtX6+oZvRrqKDgEDXrv1hXbz+Jq1sC4ozZbNaYoT8qVeo06vT5128cW7pcRXX58hs9fPBA333zhepVKaUubZvKzt5eTVu2kyTZ29u/8RyIPzG+LAGIqmTJk0uSPsjopBQpw28bZ2VlpWw5c+vOrRu6e/uWdm7ZoAwZndS4Rbtw48pUrKpiJcvq6MF9unblorJkyxln9QPxaeP+89px9JKqFs+pHM5pdOnmI41f/KdW7fJWrdK5ldQ2iY743NKeE1c07LPqkv55+Oz/fd++kvq3ryQ//xdq+v1i7T5+JU7vA4grq5cv1pmTxzVs7BQlTZbsreObtvpE5SpV06E/9ygoKFBueQuoUNEPLWt0U6dJG9slI5oSXLgtV67cOx9jMpm0Z8+eWKgGsSljJhdZWVsrODjy2aSQv7dZSZo0qYJfvJBLlsgfUsySPaeOHtyn+3fvEG5hKNbWVqpQOJtMJmn7kUsR+q/dfSpJSueYTJduvnyxzpXbjzVt5aFw44q6OSkszCyfq+F3R5jYs54+bfihHj71l0ef33XY+2Ys3QkQ/3Zv3yJJ+u6bLyLt7/lFR0nS7ys3Wh5mdnJ2UaMmLcKNO+d9ViaTSVmy5YjFavE+Ely4bdu2rSZOnKjQ0FDlzZtXDg4O8V0SYomtnZ1yu+XTubOndevGNTm5ZLH0hYaE6Mpf55XCMZVSOqZWEhsb3bx+NdLz3LpxTZKUOk26OKkbiEsrRrSQr/8LZfcYo7Cw8Gv83HN+oLAws67cfqyhXaurff2icm85SQ+e+lvGZEjtoNIFs+jYuVt67BtgaR/5RU192vBD3bz3TPV6zmd3BBhejboNVajohxHaDx/YJ2+v06pRp4EyZnJS8hQpNH3SWG1Ys0K/LV2rVKnTWMY+evhQXqeOyzVvfqXkRVUJVoILt507d5abm5u++OILpU2bVjNnzozvkhCLatZvrHNnT2vGhFHqP3yckiR5+eTqH0vm68H9u2rYtJXskyZViTIV9OeubVq7YpHqf/TP36KPHz6gw3/uVuas2ZWdLVlgMKGhYVq921vNq7vrmxZlNeb3vZa+Txt+qGJ5nbV+3znde/xcZ6/cU+oUSdWx4YcaOW+3JMnKyqSxX9eRrY11uGPrlnXTV81K68ETf1X/ao4u33oc5/cGxLVa9RpG2u7n5ytvr9OqWbehChcrLknKliOn/Hx9te6P5WrdobOkl1uITR47XCEhIWrRpkOc1Y13F6Vwe/jw4bcP+hdf34gbiUdVxYoV1a9fPw0dOlTLli1TkyZNon0uJGzV6jTUoT9368CeHfqqQ3MVK1lWN65e1pEDe+WcOatafPLylbqfftlb5729NGPCKB3at0s5c+fVrZvXdHDvTtnZJ1X37wazrzIM6fupW1SuUFYN7lJN5Qtnk9eluyqUO5OqfJhDl2891pdj1kmSFm85rS6NiuuHDpVVOHdGXbr5WNVK5JR7royas+6YVu/2tpzzx05VJEmnL95Ry5qRPzG+bNuZSF/yAPwXVKtZV2tWLNHcmVP013lvZXLOrCMH/9Slv86rdn0PlavENqcJmckchb0s8uTJ887BwWw2y2Qyydvb++2DX6Np06a6c+eOtm/friRJYneS+fxd/7cPQqwIDQnR2pWLtXndH7pz64ZSpHRUqXKV1arjZ5Y3k0kv9xdcPHeGDv25W48fPlCKlClVqFhJtWjfJcIbzhA3CjUZFd8l/Cd8kCa5BnSorDplXJUuVTLdfuCr1bu9NWLebj169s9SA8fk9vqxY2XVKeOmtI7JdOHGQ81cdVhz1x+3bFvkmNxedzb0e+s1m363WGv3+sTaPf3XXVj9bXyXgL9NGTdSK5f8rp+nzLLM3EqSn+8zzZk+Rfv37tLTp4/lkjmrGjRuqtoNGod7DTzilkvqN+/XLUUx3Pbr1y/as2LDhw+P1nFxjXALvDvCLRA9hFsgeqISbqM0HTpixIj3LgYAAACIbcyrAwAAwDAItwAAADAMwi0AAAAMg3ALAAAAwyDcAgAAwDAItwAAADCM93ozQkhIiPbt2ycfHx89efJEffv21blz5+Tg4CAXF5eYqhEAAACIkmjP3B48eFDVqlVT165dNW7cOM2dO1eStHHjRtWsWVOzZs2KqRoBAACAKIlWuPX29lbnzp0VEBCgLl26qEaNGpa+woULK126dBozZoy2b98eY4UCAAAAbxOtcDtx4kTZ2dlp5cqV6t69u1xdXS19lSpV0rJly+To6Kg5c+bEWKEAAADA20Qr3B49elS1atWSs7NzpP0ZMmRQ7dq1deHChfcqDgAAAHgX0Qq3QUFBSpYs2RvHWFtbKygoKFpFAQAAANERrXCbM2dO7du3T2FhYZH2BwcHa+/evcqePft7FQcAAAC8i2iF2yZNmujChQvq16+fHj9+HK7v4cOH6tWrl65evarGjRvHSJEAAABAVERrn9sWLVro+PHjWrNmjdauXSs7OztJUpUqVXTnzh2FhYWpWrVqatWqVYwWCwAAALxJtF/iMGrUKFWuXFnLly/X2bNnFRISIj8/PxUrVkweHh7M2gIAACDOvdcbymrXrq3atWvHVC0AAADAe4n2G8oAAACAhCZaM7ceHh5RGmcymbRy5croXAIAAAB4Z9EKt97e3m8d4+TkpJQpU0bn9AAAAEC0RCvc+vj4RNoeGBioa9euaerUqTp16pSmT5/+XsUBAAAA7yJG19za29vL1dVVY8eOVYoUKTR69OiYPD0AAADwRrHyQJnJZFLZsmW1Z8+e2Dg9AAAAEKlY2y3h+vXrevHiRWydHgAAAIggRtfcms1m+fv7a+fOndq6datKly79XsUBAAAA7yJa4bZRo0YymUyv7TebzUqaNKm++eabaBcGAAAAvKsYD7c2NjbKkSOH6tevr7Rp075XcQAAAMC7iFa4bdasmfLnzy9bW9uYrgcAAACItmg9UPbVV1/pq6++iulaAAAAgPcSrXD77Nkz5cqVK6ZrAQAAAN5LtMJt1apVtWXLFj169Cim6wEAAACiLVprbosXL65Dhw6patWqKlq0qFxcXGRvbx9hnMlkUr9+/d67SAAAACAqTGaz2fyuB+XJkydqJzeZ5O3t/c5FxYfzd/3juwQg0SnUZFR8lwAkShdWfxvfJQCJkktqu7eOidbM7bx586JzGAAAABCrohRuq1atqnbt2qlt27aSpBIlSsRqUQAAAEB0ROmBsps3b+rZs2exXQsAAADwXqK1WwIAAACQEBFuAQAAYBhRfqDM19dXt27deucLODk5vfMxAAAAQHREOdzOmzfvnXdJMJlMOnv27DsXBQAAAERHlMNtpkyZ5OzsHJu1AAAAAO8lyuG2cePG6tatW2zWAgAAALwXHigDAACAYRBuAQAAYBiEWwAAABhGlMJtt27dVLJkydiuBQAAAHgvUXqgjAfJAAAAkBiwLAEAAACGQbgFAACAYRBuAQAAYBiEWwAAABgG4RYAAACGQbgFAACAYRBuAQAAYBiEWwAAABgG4RYAAACGQbgFAACAYRBuAQAAYBiEWwAAABgG4RYAAACGQbgFAACAYRBuAQAAYBiEWwAAABgG4RYAAACGQbgFAACAYRBuAQAAYBiEWwAAABgG4RYAAACGQbgFAACAYZjMZrM5votICAJD4rsCIPEJeBEa3yUAiZJT2a/juwQgUQo4PvmtY5i5BQAAgGEQbgEAAGAYhFsAAAAYBuEWAAAAhkG4BQAAgGEQbgEAAGAYhFsAAAAYBuEWAAAAhkG4BQAAgGEQbgEAAGAYhFsAAAAYBuEWAAAAhkG4BQAAgGEQbgEAAGAYhFsAAAAYBuEWAAAAhkG4BQAAgGEQbgEAAGAYhFsAAAAYBuEWAAAAhkG4BQAAgGEQbgEAAGAYhFsAAAAYBuEWAAAAhkG4BQAAgGEQbgEAAGAYhFsAAAAYBuEWAAAAhkG4BQAAgGEQbgEAAGAYhFsAAAAYBuEWAAAAhkG4BQAAgGEQbgEAAGAYhFsAAAAYBuEWAAAAhkG4BQAAgGEQbgEAAGAYhFsAAAAYBuEWAAAAhkG4BQAAgGEQbgEAAGAYhFsAAAAYBuEWAAAAhkG4BQAAgGEQbgEAAGAYhFsAAAAYBuEWAAAAhkG4BQAAgGEQbgEAAGAYhFsAAAAYBuEWAAAAhkG4BQAAgGEQbgEAAGAYhFsAAAAYBuEWAAAAhkG4BQAAgGEQbgEAAGAYhFsAAAAYBuEWAAAAhkG4BQAAgGEQbgEAAGAYhFsAAAAYBuEWAAAAhkG4BQAAgGEQbgEAAGAYhFsAAAAYRpL4LgD4t0L53d465tc581S8RMk4qAZImB4+uK9fp03Rvr279ejhA6V0dFTxkqXV+bMv5eySWZLUqE413bl9643n6T9oqOo18IiLkoF4NbyHh7q3raoanSZoz9ELkY5JZm+rYyu+19odp9R7zIoI/UmSWKlLkwr6xKOMcrik0+Nnz7V252kNmbZeD588j+1bQBQRbpHgdP28W6Ttjx4+1NIli5QmbVplz54jjqsCEo6HD+6rQ5tmunvnjkqUKqPqNWvr6pUr2rxxvfbv26Nff1ukLFmzqXmrtvL1fRbh+KCgIC2cN0e2tnbKl79APNwBELc+zJ9V3VpWeuMYa2srzR3WTlmd0r52zIyBrdWibgkd9bqq6Ut3K7tLOnVuUk61y+dX2VajCLgJBOEWCc5nX3wZafvX3T6TyWTSsBGjlS59+jiuCkg4fp02RXfv3NFX3/RRyzafWNo3rl+jQf37aeLY0RozYYqat2ob6fE/jxyqsLAwfdPnO+XImTuOqgbih00Sa00b2EpJkli/dkzqlMk0b0R7VSud97VjqpbKoxZ1S+iPrcfVsvcsS3vHj8pqcv8W6vlJdX03flVMlo5oYs0tEoX169Zo547tavxRE5UuUza+ywHi1c4d25Q6dZoI4bV23QZyyZxZB/fvVVhYWKTHnjh+VMuXLFSJUmXUwOOjuCgXiFd9O9VUrizpte2AT6T9TWsV0/GV/VWtdF5t3e/92vPkzZFRdx4805g5W8K1L/U8Kkkq6Z495orGe2HmFgleUFCQJo0fpxQpUuirHt/EdzlAvAoNDdUnHTvLOkkSWVlFnJ+wsbFVcHCwQkJCZGtrG6F/4thRsrK2Vs++38dFuUC8KpDbSb071NCoWZuVKkVSVS2VJ8KYjh+VU2BQsBp/NU1+/kGvnb2dvHCnJi/cGaHdLdsHkqR7j3xjtHZEX4KbuQ0ICNDRo0d18uRJvXjx4rXj7t+/r71798ZhZYgvSxYt1O3bt/RJh05KlSp1fJcDxCtra2s1a9lGHzdtEaHvyuVLunrlslwyZ4402O7YtkVnz5xWvQYeypqNWSYYm5WVSdN+bKW/rt3XqFmbXjtu2IyNKuQxWBv3nHmn86dwsFedCgU0b0R7Bb0I1oT52963ZMSQBDVzO3fuXE2cOFEBAQGSpGTJkql9+/b67LPPZG0dfq3M/v371bdvX3l7v/5HCEj8QkNDtXDBPDk4OKhp85bxXQ6QYIWFhWnMiCEKCwtTw8ZNIx2zaMFvsrKyUqu27eO4OiDu9WhbVYXzZFbVDuMUHBL62nG7Dp9/53NXKuGqjdO/kiSFhISq7bdzdODk5WjXipiVYGZuV69erREjRsjNzU3ffvutOnfurKRJk2rKlCnq0KGDnj/nCcT/op07tuv27Vtq/HFTpUyZMr7LARIks9msEUMG6sihA8qbr4Cat2oTYcw5n7M6deKYKlSqqixZs8V9kUAcypUlg77vUkczlu3RwVMxHzqDgkI0du4W/bZqv54HvNBvw9qrdX22p0woEszM7dy5c1WsWDH9/vvvlrauXbvq+++/14YNG9SxY0fNnj1byZIli8cqEdfWrlklSfq4SeQzUcB/XUhIiIYP/kHr16ySs0tmjRo3WTY2EZckbFy3RpLU6KMmcV0iEOem/dhS9x/7acDE1bFy/v0nL2n/yUuSpGEzNmjv7300uX9z7Th4TjfvPYmVayLqEszM7cWLF1WnTp1wbUmTJtXYsWPVrFkznThxQp999tkb1+HCWIKCgnTgzz+V29VV2djXFoggMCBAfXp00/o1q5Q5S1ZNmTFH6TNkiHTs3t07lNLRUR+WKBXHVQJxq2uzCipbNJe+GrZYzwNiPzNcu/1Yk3/fKTtbG1Uv+/qtxBB3EszMrZ2d3WuXHgwaNEh+fn5av369unfvrkmTJsVxdYgPRw4fUkCAv6pVrxnfpQAJzrNnT9WjWxd5nT4l1zx5NX7KDKVJE/nm81evXNaN69dVr6GHkiRJML/tA7HCo1oRSdKqSZ9H2r/5168lSW51ftC124+ifN6i+bIoV5b0lq2//t+r86RLlfxdy0UsSDC/yxUuXFgLFy5U48aNlS5dugj9I0eO1NOnT7Vjxw59/fXXKl26dDxUibh0+tRJSVKRosXiuRIgYQkKClLPrz6X1+lTKlKsuMaMnyKH5K//Q/XM399LhQoXjasSgXgzf80B7T4S8fW6NcrkVQn37Jq/5oCu3nqkp74B73Ten75soKql8sjrr9vy+iv8a63dXZ0lSZeuP4h+4YgxCSbcfvnll2rbtq1q1qypChUqqHfv3nJycrL0J0mSRJMnT9YXX3yhrVu3aufOnfFXLOKEj/dZSVLefPnjuRIgYZk6abxOnzyugu6FNW7ydNnb279x/PlzL3eVccubLy7KA+LVgrUHI21PlSLp3+H2oPYcjRh+32bF5mOqWiqPBn/VQB93n66wMLMkqUjezOrStILuPHgmz71e71U7YkaCCbfu7u5asWKFRo4cqV27dqlXr14Rxtjb22v69OmaMGGC5syZEw9VIi5dv35d9vb27JIA/J+HD+5rxdKFkqRs2XNo/txfIx3Xtv2nsrOzkyTduHFdkpQufeTrcQG83W+r96tx9SKqXb6ADizqp20HfOSUwVENqxRSSGiYPvlurvwDeS4oIUgw4VaScubMqRkzZigsLCzSN+9IL2dwe/bsqbZt2+rYsWNxXCHi0tMnj5U8eYr4LgNIUM6cPqng4GBJ0trVK187rnmrtpZw++zJE0ni+wl4D2FhZjX+apq++aSaWtYtoc9bVNQzv0Ct23laQ6dvkPelO/FdIv5mMpvN5vguIiEIDInvCoDEJ+DF6zdGB/B6TmW/ju8SgEQp4Pjkt45JMFuBAQAAAO+LcAsAAADDINwCAADAMAi3AAAAMAzCLQAAAAyDcAsAAADDINwCAADAMAi3AAAAMAzCLQAAAAyDcAsAAADDINwCAADAMAi3AAAAMAzCLQAAAAyDcAsAAADDINwCAADAMAi3AAAAMAzCLQAAAAyDcAsAAADDINwCAADAMAi3AAAAMAzCLQAAAAyDcAsAAADDINwCAADAMAi3AAAAMAzCLQAAAAyDcAsAAADDINwCAADAMAi3AAAAMAzCLQAAAAyDcAsAAADDINwCAADAMAi3AAAAMAzCLQAAAAyDcAsAAADDINwCAADAMAi3AAAAMAzCLQAAAAyDcAsAAADDINwCAADAMAi3AAAAMAzCLQAAAAyDcAsAAADDINwCAADAMAi3AAAAMAzCLQAAAAyDcAsAAADDINwCAADAMAi3AAAAMAzCLQAAAAyDcAsAAADDINwCAADAMAi3AAAAMAzCLQAAAAyDcAsAAADDINwCAADAMAi3AAAAMAzCLQAAAAyDcAsAAADDINwCAADAMAi3AAAAMAzCLQAAAAyDcAsAAADDINwCAADAMAi3AAAAMAzCLQAAAAyDcAsAAADDINwCAADAMAi3AAAAMAzCLQAAAAyDcAsAAADDMJnNZnN8FwEAAADEBGZuAQAAYBiEWwAAABgG4RYAAACGQbgFAACAYRBuAQAAYBiEWwAAABgG4RYAAACGQbgFAACAYRBuAQAAYBiEWwAAABgG4RYAAACGQbgFAACAYRBuAQAAYBiEWyRYt27dUo8ePVSqVCkVK1ZMX3zxha5fvx7fZQGJxowZM1S2bNn4LgNIFE6dOqVPP/1UH374oQoWLKhGjRpp1apV8V0WosFkNpvN8V0E8G9PnjzRxx9/LD8/P7Vr1062traaPXu2rK2ttWrVKqVJkya+SwQStF27dumLL76Qo6Oj9u3bF9/lAAnaxYsX1bhxYzk6Oqply5ZycHDQhg0bdOzYMfXr10/t27eP7xLxDgi3SJDGjx+vadOmafny5SpQoIAk6fz582rUqJHatWunvn37xnOFQMJkNpv1+++/a8SIEQoODla6dOkIt8BbdO7cWYcPH5anp6c++OADSVJYWJhatmypc+fOae/evXJwcIjnKhFVLEtAgrRu3ToVLlzYEmwlydXVVaVKldK6devisTIgYWvWrJkGDx6skiVLKn/+/PFdDpDghYaG6vDhwypfvrwl2EqSlZWVateuLX9/f3l7e8djhXhXhFskOE+fPtX169fDBdtX8ufPr3v37unevXvxUBmQ8N26dUs//fSTfv31V2aagCiwsrLSmjVr1KdPnwh9jx49kiRZW1vHdVl4D0niuwDg3+7evStJ4f4G/UqGDBkkSbdv37b8N4B/bN++Xba2tvFdBpBomEwmZc6cOUK7v7+/VqxYoWTJkilfvnzxUBmii5lbJDjPnz+XJCVNmjRCn729vaSXv+kAiIhgC7w/s9ms/v376/79+2rfvr3s7OziuyS8A8ItEpxXzziaTKbXjnlTHwAA0WU2mzVw4ECtX79eJUqU0GeffRbfJeEdsSwBCU6yZMkkSQEBARH6AgMDJUnJkyeP05oAAMYXHBysfv36ad26dXJ3d9fUqVNlY2MT32XhHRFukeA4OztLku7fvx+h79WDZJGtxwUAILoCAgL05Zdfas+ePSpRooSmTp3KREoixbIEJDgpUqRQlixZ5OXlFaHPy8tLGTNmVPr06eOhMgCAEQUHB6tbt27as2ePKleurF9//ZVgm4gRbpEg1apVS0ePHg0XcM+fP68DBw6oXr168VgZAMBoJk6cqL1796pKlSqaNGkSD5AlcryhDAnSkydPVL9+fQUHB6tjx46ysrLSnDlzZGNjoxUrVvD6XSAK2rRpo0uXLvGGMuAN7t27pypVqshsNuu7776LdMa2dOnSbD+ZiLDmFglSqlSptHDhQg0fPly//PKLbG1tVaJECfXp04dgCwCIMceOHVNwcLAk6aeffop0zMyZMwm3iQgztwAAADAM1twCAADAMAi3AAAAMAzCLQAAAAyDcAsAAADDINwCAADAMAi3AAAAMAzCLQAAAAyDcAsAAADDINwCSHQmTZokNze3CP/kz59fJUuWVJs2bbR69eo4renZs2dyc3NTmzZtLG0rV66Um5ub5s6dG61zrlu3TtevX4+hCv/RsGFDubm5vXXcq8955cqVMXr9V+fdunVrjJ734MGDcnNz09ChQ2P0vAASF16/CyDRqlq1qvLmzWv5OiQkRI8ePdLGjRvVp08fXbp0ST169Ii3+vLmzatu3bqpcOHC73zs6NGj9euvv2rVqlUxXhcAGBnhFkCiVa1aNTVu3DhCe8eOHeXh4aGZM2eqadOmcnZ2jofqXobb/w/f7+Lhw4cxXA0A/DewLAGA4WTLlk1Vq1ZVaGio9u7dG9/lAADiEOEWgCF98MEHkqQnT55I+mf968aNG9WxY0cVLFhQlStXtqxp9fPz05gxY1StWjUVKFBA5cuX148//hjpDOqNGzfUq1cvlSlTRkWKFFG3bt1069atCONet+bWx8dHPXr0UNmyZVWkSBF5eHho+fLlMpvNkqQqVarojz/+kCQ1atRIVapUsRxrNpu1aNEieXh4yN3dXcWLF1fXrl119uzZCNcPDAzU2LFjVaVKFbm7u6tp06Y6fPjwu3+YUXTz5k39+OOPqlatmgoWLKgiRYqocePGWrRoUaTjAwMDNWzYMJUuXVqFCxdWmzZtdPDgwUjHbty4Uc2bN1eRIkVUtGhRtWvXTgcOHHhrTSEhIZo8ebLq16+vwoULq0SJEurYsaP279//XvcKIOFiWQIAQ7p27Zqkf0LuK0OGDFGGDBnUpk0b3bhxQ5kzZ5avr69atmyp8+fPq3Tp0qpRo4Zu3LihpUuXas+ePVq8eLEyZMggSbpz546aN2+uBw8eqEqVKnJyctKePXvUqVOnKNW1f/9+de3aVaGhoapataqcnJy0c+dOff/997p165a++uortW3bVn/88Yd8fHzUrFkz5ciRw3J83759tXr1auXOnVvNmzdXQECAJfhNnz5dpUuXliSFhYXp008/1aFDh+Tu7q7q1avr9OnT6tChg5ImTRoTH3E4N27c0Mcff6yAgABVr15dmTJl0t27d7Vp0yYNHDhQoaGhat26dbhjRowYoeDgYNWrV0/Pnz+Xp6en2rdvr19++UWVKlWyjJswYYJ++eUXOTs7y8PDQyaTyTJ2xIgRatiw4WvrGjx4sBYvXqwSJUqoQoUK8vX11YYNG9SxY0fNmTNHJUuWjPHPAkA8MwNAIjNx4kSzq6urecWKFZH2nzp1ypwvXz6zu7u7+eHDh2az2WxesWKF2dXV1VyhQgWzv79/uPEDBw40u7q6mhcsWBCufevWrWZXV1fzV199ZWnr06eP2dXV1bxy5UpL2/Pnz82tW7c2u7q6mlu3bm1pf3XNOXPmmM1mszkkJMRcuXJlc8GCBc3Hjh2zjAsMDDTXr1/fnC9fPvODBw/MZrPZ3LdvX7Orq6v57NmzlnEbNmwwu7q6mr/55htzcHCwpf3atWvmEiVKmMuXL28OCgoym81m8/Lly82urq7mb7/91hwaGmoZO3LkSLOrq6vZ1dX1DZ/wS2/7nP/fgAEDzK6uruZ9+/aFaz958qTZ1dXV3KxZswjnLV68uPn69euWdi8vL3OhQoXMlSpVMoeEhFiOd3NzM7du3Trcr9ujR4/M1atXNxcqVMjya3zgwAGzq6ureciQIWaz2Wz29fU158mTx9yqVatwNZ06dcrs6upq/vLLL996XwASH2ZuASRaW7du1c2bNy1fh4SE6PLly9q5c6dCQkL03XffKU2aNOGOqVChQriZy5CQEK1atUq5c+dWq1atwo2tWrWqihYtqi1btsjPz0+2trbavHmzcufOLQ8PD8u4ZMmSqVevXmratOkb6z1x4oRu3ryppk2bqkiRIpZ2Ozs79evXTydPnlRQUNBrj1++fLkk6fvvv1eSJP/89p05c2Y1b95c06ZN059//qlKlSpp/fr1MplM6tmzp6ys/lmB1r17dy1dulS+vr5vrPVdNWjQQIUKFVKZMmXCtbu7u8ve3j7S5R1t27aVi4uL5et8+fKpQYMGWrJkiY4cOaKSJUtalmv06dMn3K9b6tSp9emnn6p///7auHFjhF876eXstdls1u3bt3X//n2lT59eklSwYEFt3bpVGTNmjKnbB5CAEG4BJFrbtm3Ttm3bLF/b2NgoVapUKlu2rFq1aqVy5cpFOOb/w5QkXb58Wf7+/goNDdWkSZMijA8KClJoaKjOnTunVKlSyd/fXwUKFIgwrkCBArKxsXljvT4+PpIU6dZgZcqUiRAM/83Ly0t2dnb6/fffI/RdvnxZkuTt7a1KlSrJx8dHTk5OSps2bbhxtra2yp8/f5TWq76LDz/8UB9++KGePHkib29vXbt2TZcvX9aJEycsn+G/FS1aNEKbu7u7lixZIh8fH5UsWVJeXl6SpM2bN2vnzp3hxt65c0fSy3uOTMqUKVWnTh2tX79elStXVpEiRVShQgVVrlxZuXLles87BpBQEW4BJFrDhw+PdCuwN7Gzswv39bNnzyRJly5d0uTJk1973NOnT2UymSRJDg4OEfqtra2VPHnyN1771bXeNu51fH19LQ9IvanOV9f6d7B9xdHRMVrXf5OnT59q+PDhWrdunYKDg2UymeTs7KxSpUpF+rCbpEjre/XZ+vv7S5JlhnnGjBlvvPbrjBw5UgUKFNDKlSt16NAhHTp0SGPGjFGBAgU0ZMiQaG/VBiDhItwC+E97FaYaNmyoUaNGvXHsxYsXJSnSH+mbzWYFBAS88fhkyZJJkp4/fx6hLzg4WGazWba2tm883sHBIcIMZmRSpkz52qUHr4JjTOrdu7d27dql5s2bq2HDhnJ1dbWE+LVr10Z6TGT13bt3T9I/ATxZsmSytrbWyZMn3zozHhkbGxt16NBBHTp00K1bt7Rv3z55enpq79696tKli7Zt2xat8wJIuNgKDMB/Wvbs2WVraysvLy/LVlz/b+7cufrll1/0+PFjZcmSRSlSpNDx48cjjPvrr78UGBj4xmu5urpKkk6dOhWhb+PGjSpUqJDljWSvZon/n5ubm+7cuaP79+9H6Nu5c6fGjRtnWfqQP39+3b59O8IWZaGhoa/9MX50PXv2TLt27VKBAgU0aNAgFS1a1BJsb9y4oaCgoEg/29OnT0doO3HihCRZln64ubm9tuYTJ05ozJgxOnLkSKR1Xb9+XWPHjtWOHTskSU5OTmrSpIlmzZqlUqVK6e7du7px40a07hlAwkW4BfCfZmdnpzp16uivv/7SnDlzwvUdPHhQo0aN0ooVK+To6CgbGxvVq1dP165dCzf2xYsX+vnnn996reLFiytTpkxavXp1uLD24sULzZ07V9bW1patvF49MBYcHGwZ5+HhIbPZrMGDB+vFixeW9nv37unHH3/UjBkzLDPRrx54e7Xd1iuzZs3SgwcPovz5RIWNjY2srKz07NmzcHUFBgZq8ODBEe7jlfnz5+vRo0eWr48cOSJPT0/lzp1b7u7u4e5j2LBh8vPzs4z18/PTwIEDNXPmzEjX80qSvb29Zs6cqQkTJoSr68WLF7p//75sbW0tD5kBMA6WJQD4z+vbt6+OHz+ukSNHatu2bXJ3d9fdu3e1efNmJUmSRMOGDbPsONCjRw/t379fI0aM0N69e5UzZ07t379fT548ibCe999enatLly5q3ry5qlevrrRp02rnzp26cuWKvv32W8u+vK/+PWLECJUpU0bdunVT48aNtX37dm3atEnnzp1T+fLlFRISoo0bN+rJkyfq2bOnMmfOLEmqU6eONm3aJE9PT12+fFmlS5fWX3/9pQMHDsjZ2TncLhNvM2PGDMtLJf6tVatWqlWrlqpXr65NmzapSZMmKlu2rPz9/bVjxw49ePBAjo6O8vX1VVhYWLidG5IkSaKGDRuqTp06evjwoTw9PWVvb6/hw4dbxpQqVUpt2rTR/PnzVbduXVWsWFG2trbaunWrbt++rebNm792r9r06dOrXbt2mjNnjurVq6eKFSvKyspKe/bs0cWLF/X5559He/0zgISLcAvgPy9NmjRaunSppk+fri1btmj+/PlKkyaNqlSpos8//1x58uSxjHV0dNSiRYs0YcIEbdu2TUeOHFHRokU1fvx4NWvW7K3XKlOmjBYtWqTJkydr165dCggIUK5cuTRy5Eg1atTIMq5ly5Y6duyYjhw5oosXL6p9+/ZycHDQxIkT9fvvv2vlypVatmyZ7O3tlStXLrVv317VqlULd62xY8eqQIECWr58uRYtWqRs2bJp8uTJWr58+TuF28uXL1t2Y/i3qlWrSno5s5oxY0Zt3bpVCxYsUPr06VWwYEF17txZ69at02+//aaDBw9aZqZfHbN69WqtXLlSISEhKlu2rHr27GlZvvFK//79VbBgQS1atEhr1qyRtbW1smfPri+//DLclmyR6d27t7Jmzaply5bpjz/+UGhoqHLlyqURI0a89VgAiZPJHNlCKAAAACARYs0tAAAADINwCwAAAMMg3AIAAMAwCLcAAAAwDMItAAAADINwCwAAAMMg3AIAAMAwCLcAAAAwDMItAAAADINwCwAAAMMg3AIAAMAwCLcAAAAwDMItAAAADON/tPpaJWwB0cYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predictions_ensemble))\n",
    "\n",
    "# Plot confusion matrix with blue colors\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set(font_scale=1.2)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate on test data\n",
    "# Bert_model.eval()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# input_ids_test, attention_masks_test, y_test = Bert_tokenize(X_test, y_test)\n",
    "# test_dataset = TensorDataset(input_ids_test, attention_masks_test, y_test)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# total_test_loss = 0\n",
    "# correct_test = 0\n",
    "# total_test = 0\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch in test_dataloader:\n",
    "#         input_ids, attention_mask, labels = batch\n",
    "#         input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "#         labels = labels.reshape(-1)  # Reshape labels once\n",
    "\n",
    "#         outputs = Bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         logits = outputs\n",
    "#         loss = criterion(logits, labels)\n",
    "#         total_test_loss += loss.item()\n",
    "\n",
    "#         _, predicted = torch.max(logits, 1)\n",
    "#         correct_test += (predicted == labels).sum().item() \n",
    "#         total_test += labels.size(0)\n",
    "#         accuracy_test = correct_test / total_test\n",
    "#         print(f'Test Loss: {total_test_loss:.4f} - Test Accuracy: {accuracy_test:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.1097 - Test Accuracy: 0.3333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate on test data\n",
    "RoBerta_model.eval()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "input_ids_test, attention_masks_test, y_test = RoBertaTokenizer(X_test, y_test)\n",
    "test_dataset = TensorDataset(input_ids_test, attention_masks_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "total_test_loss = 0\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        labels = labels.reshape(-1)  # Reshape labels once\n",
    "\n",
    "        outputs = RoBerta_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs\n",
    "        loss = criterion(logits, labels)\n",
    "        total_test_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        correct_test += (predicted == labels).sum().item() \n",
    "        total_test += labels.size(0)\n",
    "        accuracy_test = correct_test / total_test\n",
    "        print(f'Test Loss: {total_test_loss:.4f} - Test Accuracy: {accuracy_test:.4f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_bert' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming model_bert and model_roberta are your loaded models\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel_bert\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      3\u001b[0m model_roberta\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Prepare data for both models\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_bert' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming model_bert and model_roberta are your loaded models\n",
    "model_bert.eval()\n",
    "model_roberta.eval()\n",
    "\n",
    "# Prepare data for both models\n",
    "input_ids_bert, attention_masks_bert, _ = Bert_tokenize(X_test, y_test)\n",
    "input_ids_roberta, attention_masks_roberta, _ = RoBertaTokenizer(X_test, y_test)\n",
    "\n",
    "# Convert to appropriate device\n",
    "input_ids_bert, attention_masks_bert = input_ids_bert.to(device), attention_masks_bert.to(device)\n",
    "input_ids_roberta, attention_masks_roberta = input_ids_roberta.to(device), attention_masks_roberta.to(device)\n",
    "\n",
    "# Get logits from both models\n",
    "with torch.no_grad():\n",
    "    outputs_bert = model_bert(input_ids=input_ids_bert, attention_mask=attention_masks_bert)\n",
    "    logits_bert = outputs_bert.logits  # Adjust depending on your model's output\n",
    "\n",
    "    outputs_roberta = model_roberta(input_ids=input_ids_roberta, attention_mask=attention_masks_roberta)\n",
    "    logits_roberta = outputs_roberta.logits  # Adjust depending on your model's output\n",
    "\n",
    "    # Average the logits\n",
    "    averaged_logits = (logits_bert + logits_roberta) / 2\n",
    "\n",
    "    # Apply softmax to get probabilities\n",
    "    probabilities = torch.nn.functional.softmax(averaged_logits, dim=1)\n",
    "\n",
    "    # Get the predicted classes\n",
    "    _, predicted_classes = torch.max(probabilities, 1)\n",
    "\n",
    "# Evaluate the predictions as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
