{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Model\n",
    "\n",
    "\n",
    "----\n",
    "```\n",
    ": 26.05.24\n",
    ": zachcolinwolpe@gmail.com\n",
    "```\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zachwolpe/miniforge3/envs/mlxgo/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/zachwolpe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importing neccessary libraries \n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertPreTrainedModel, BertModel,AdamW\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import argparse\n",
    "import logging\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import torch.optim as optim\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/zachwolpe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from Roberta_classifier import (RobertaWithActivationAndRegularization, RoBertaTokenizer)\n",
    "from ML_training_code import (generate_K_Fold_data, torch_tensorize, plot_training_validation, training_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bert_classifier import (BertWithActivationAndRegularization, Bert_tokenize)\n",
    "from ML_training_code import (generate_K_Fold_data, torch_tensorize, plot_training_validation, training_loop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Pre-Trained Models\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config ---------------------------------------->>\n",
    "PATH_TO_DATA = '../data/train_test/'\n",
    "SAVE_LOC = '../model-artifacts/'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEBUG_MODE = True\n",
    "BATCH_SIZE = 10\n",
    "# Config ---------------------------------------->>\n",
    "\n",
    "# Load data\n",
    "X_test = pd.read_csv(PATH_TO_DATA + 'X_test.csv')\n",
    "y_test = pd.read_csv(PATH_TO_DATA + 'y_test.csv')\n",
    "\n",
    "\n",
    "# Downsample for testing\n",
    "if DEBUG_MODE:\n",
    "    X_test = X_test[:3]\n",
    "    y_test = y_test[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Load Models\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# load Bert model\n",
    "Bert_model = BertWithActivationAndRegularization(pretrained_model_name='bert-base-uncased', num_labels=3)\n",
    "Bert_model.load_state_dict(torch.load(f'{SAVE_LOC}bert_model.pth'))\n",
    "\n",
    "\n",
    "# Load RoBerta model\n",
    "RoBerta_model = RobertaWithActivationAndRegularization(pretrained_model_name='roberta-base', num_labels=3,)\n",
    "RoBerta_model.load_state_dict(torch.load(f'{SAVE_LOC}roberta_model.pth'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Inference Engine\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inference(model, X_test, y_test, tokenizer=Bert_tokenize, criterion=nn.CrossEntropyLoss()):\n",
    "    \"\"\"\n",
    "    Perform inference on the model\n",
    "\n",
    "    Return softmax probabilities, predictions, and metrics\n",
    "    \"\"\"    \n",
    "    # evaluate on test data\n",
    "    model.eval()\n",
    "    input_ids_test, attention_masks_test, y_test = tokenizer(X_test, y_test)\n",
    "    test_dataset = TensorDataset(input_ids_test, attention_masks_test, y_test)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    total_test_loss = 0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "\n",
    "    #return items\n",
    "    softmax_prob = []\n",
    "    predictions = []\n",
    "    accuracy = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            labels = labels.reshape(-1)  # Reshape labels once\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs\n",
    "            loss = criterion(logits, labels)\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "            total_test += labels.size(0)\n",
    "            accuracy_test = correct_test / total_test\n",
    "            print(f'Test Loss: {total_test_loss:.4f} - Test Accuracy: {accuracy_test:.4f}')\n",
    "\n",
    "            # Get softmax probabilities\n",
    "            softmax_prob.append(nn.functional.softmax(logits, dim=1).cpu().numpy())\n",
    "            predictions.append(predicted.cpu().numpy())\n",
    "            accuracy.append(accuracy_test)\n",
    "\n",
    "        # flatten the list\n",
    "        softmax_prob = np.concatenate(softmax_prob)\n",
    "        predictions = np.concatenate(predictions)\n",
    "        accuracy = np.mean(accuracy)\n",
    "        \n",
    "        return softmax_prob, predictions, accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Model Evaluation: Bert \n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.9103 - Test Accuracy: 0.6667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.61952966, 0.14688203, 0.23358834],\n",
       "        [0.29358956, 0.47601697, 0.23039342],\n",
       "        [0.14984523, 0.2641538 , 0.586001  ]], dtype=float32),\n",
       " array([0, 1, 2]),\n",
       " 0.6666666666666666)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(softmax_prob_Bert,\n",
    " predictions_Bert,\n",
    " accuracy_Bert) = inference(Bert_model,\n",
    "                            X_test,\n",
    "                            y_test,\n",
    "                            tokenizer=Bert_tokenize)\n",
    "softmax_prob_Bert, predictions_Bert, accuracy_Bert\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6666666666666666]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_prob\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Model Evaluation: RoBerta \n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.9103 - Test Accuracy: 0.6667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.61952966, 0.14688203, 0.23358834],\n",
       "        [0.29358956, 0.47601697, 0.23039342],\n",
       "        [0.14984523, 0.2641538 , 0.586001  ]], dtype=float32),\n",
       " array([0, 1, 2]),\n",
       " 0.6666666666666666)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(softmax_prob_RoBerta,\n",
    " predictions_RoBerta,\n",
    " accuracy_RoBerta) = inference(RoBerta_model,\n",
    "                            X_test,\n",
    "                            y_test,\n",
    "                            tokenizer=RoBertaTokenizer)\n",
    "softmax_prob_RoBerta, predictions_RoBerta, accuracy_RoBerta\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Ensemble Model\n",
    "----\n",
    "\n",
    "Ensemble methods:\n",
    "\n",
    "To ensemble your two deep learning classifiers (Bert and RoBerta models), you can average their predictions. Here's a concise guide on how to implement this in PyTorch:\n",
    "1. Load Both Models: Ensure both models are loaded and set to evaluation mode.\n",
    "2. Prepare the Data: Tokenize and prepare your data as required by both models. Since you're using different tokenizers for Bert and RoBerta, you'll need to prepare the inputs separately for each model.\n",
    "3. Get Predictions from Both Models: Pass the test data through both models to get the logits (raw model outputs before applying softmax).\n",
    "4. Average the Logits: Average the logits obtained from both models. This simple method of ensembling is often quite effective.\n",
    "5. Apply Softmax and Determine Final Predictions: Apply softmax to the averaged logits to get probabilities, and then determine the final predictions (e.g., by taking the argmax).\n",
    "Here's how you could implement this:\n",
    "\n",
    "\n",
    "\n",
    "This code snippet averages the predictions from both models. You can also explore other ensembling techniques like weighted averaging or voting based on validation set performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_inference(model1, model2, X_test, y_test, tokenizers=[Bert_tokenize, RoBertaTokenizer], criterion=nn.CrossEntropyLoss()):\n",
    "    \"\"\"\n",
    "    Perform inference on the model\n",
    "\n",
    "    Return softmax probabilities, predictions, and metrics\n",
    "    \"\"\"    \n",
    "    # Assuming model_bert and model_roberta are your loaded models\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "\n",
    "    # Prepare data for both models\n",
    "    input_ids_bert, attention_masks_bert, _ = tokenizers[0](X_test, y_test)\n",
    "    input_ids_roberta, attention_masks_roberta, _ = tokenizers[1](X_test, y_test)\n",
    "\n",
    "    # Convert to appropriate device\n",
    "    input_ids_bert, attention_masks_bert = input_ids_bert.to(device), attention_masks_bert.to(device)\n",
    "    input_ids_roberta, attention_masks_roberta = input_ids_roberta.to(device), attention_masks_roberta.to(device)\n",
    "\n",
    "    # Get logits from both models\n",
    "    with torch.no_grad():\n",
    "        logits_model1 = model1(input_ids=input_ids_bert, attention_mask=attention_masks_bert)\n",
    "  \n",
    "        logits_model2 = model2(input_ids=input_ids_roberta, attention_mask=attention_masks_roberta)\n",
    "\n",
    "        # Average the logits\n",
    "        averaged_logits = (logits_model1 + logits_model2) / 2\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probabilities = torch.nn.functional.softmax(averaged_logits, dim=1)\n",
    "\n",
    "        # Get the predicted classes\n",
    "        _, predicted_classes = torch.max(probabilities, 1)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        acc = torch.sum(predicted_classes == torch.tensor(y_test['Sentiment'])) / len(y_test)\n",
    "\n",
    "        return probabilities.cpu().numpy(), predicted_classes.cpu().numpy(), acc.item()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.4423466 , 0.21856269, 0.33909073],\n",
       "        [0.33239466, 0.40170312, 0.26590228],\n",
       "        [0.200824  , 0.2807705 , 0.51840544]], dtype=float32),\n",
       " array([0, 1, 2]),\n",
       " 0.6666666865348816)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensemble Inference\n",
    "(softmax_prob_ensemble,\n",
    " predictions_ensemble,\n",
    " accuracy_ensemble) = ensemble_inference(Bert_model,\n",
    "                                        RoBerta_model,\n",
    "                                        X_test,\n",
    "                                        y_test,\n",
    "                                        tokenizers=[Bert_tokenize, RoBertaTokenizer])\n",
    "\n",
    "\n",
    "(softmax_prob_ensemble,\n",
    " predictions_ensemble,\n",
    " accuracy_ensemble) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate on test data\n",
    "# Bert_model.eval()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# input_ids_test, attention_masks_test, y_test = Bert_tokenize(X_test, y_test)\n",
    "# test_dataset = TensorDataset(input_ids_test, attention_masks_test, y_test)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# total_test_loss = 0\n",
    "# correct_test = 0\n",
    "# total_test = 0\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch in test_dataloader:\n",
    "#         input_ids, attention_mask, labels = batch\n",
    "#         input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "#         labels = labels.reshape(-1)  # Reshape labels once\n",
    "\n",
    "#         outputs = Bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         logits = outputs\n",
    "#         loss = criterion(logits, labels)\n",
    "#         total_test_loss += loss.item()\n",
    "\n",
    "#         _, predicted = torch.max(logits, 1)\n",
    "#         correct_test += (predicted == labels).sum().item() \n",
    "#         total_test += labels.size(0)\n",
    "#         accuracy_test = correct_test / total_test\n",
    "#         print(f'Test Loss: {total_test_loss:.4f} - Test Accuracy: {accuracy_test:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      3\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 4\u001b[0m input_ids_test, attention_masks_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mRoBertaTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m TensorDataset(input_ids_test, attention_masks_test, y_test)\n\u001b[1;32m      6\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE)\n",
      "File \u001b[0;32m~/Desktop/µπ/mlxgo/deep-learning-sentiment-analysis/code/Roberta_classifier.py:48\u001b[0m, in \u001b[0;36mRoBertaTokenizer\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     46\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenized_inputs\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[1;32m     47\u001b[0m attention_masks \u001b[38;5;241m=\u001b[39m tokenized_inputs\u001b[38;5;241m.\u001b[39mattention_mask\n\u001b[0;32m---> 48\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSentiment\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m input_ids, attention_masks, y\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate on test data\n",
    "RoBerta_model.eval()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "input_ids_test, attention_masks_test, y_test = RoBertaTokenizer(X_test, y_test)\n",
    "test_dataset = TensorDataset(input_ids_test, attention_masks_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "total_test_loss = 0\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        labels = labels.reshape(-1)  # Reshape labels once\n",
    "\n",
    "        outputs = RoBerta_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs\n",
    "        loss = criterion(logits, labels)\n",
    "        total_test_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        correct_test += (predicted == labels).sum().item() \n",
    "        total_test += labels.size(0)\n",
    "        accuracy_test = correct_test / total_test\n",
    "        print(f'Test Loss: {total_test_loss:.4f} - Test Accuracy: {accuracy_test:.4f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming model_bert and model_roberta are your loaded models\n",
    "model_bert.eval()\n",
    "model_roberta.eval()\n",
    "\n",
    "# Prepare data for both models\n",
    "input_ids_bert, attention_masks_bert, _ = Bert_tokenize(X_test, y_test)\n",
    "input_ids_roberta, attention_masks_roberta, _ = RoBertaTokenizer(X_test, y_test)\n",
    "\n",
    "# Convert to appropriate device\n",
    "input_ids_bert, attention_masks_bert = input_ids_bert.to(device), attention_masks_bert.to(device)\n",
    "input_ids_roberta, attention_masks_roberta = input_ids_roberta.to(device), attention_masks_roberta.to(device)\n",
    "\n",
    "# Get logits from both models\n",
    "with torch.no_grad():\n",
    "    outputs_bert = model_bert(input_ids=input_ids_bert, attention_mask=attention_masks_bert)\n",
    "    logits_bert = outputs_bert.logits  # Adjust depending on your model's output\n",
    "\n",
    "    outputs_roberta = model_roberta(input_ids=input_ids_roberta, attention_mask=attention_masks_roberta)\n",
    "    logits_roberta = outputs_roberta.logits  # Adjust depending on your model's output\n",
    "\n",
    "    # Average the logits\n",
    "    averaged_logits = (logits_bert + logits_roberta) / 2\n",
    "\n",
    "    # Apply softmax to get probabilities\n",
    "    probabilities = torch.nn.functional.softmax(averaged_logits, dim=1)\n",
    "\n",
    "    # Get the predicted classes\n",
    "    _, predicted_classes = torch.max(probabilities, 1)\n",
    "\n",
    "# Evaluate the predictions as needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
